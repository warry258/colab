{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title æŒ‚è½½Googleç¡¬ç›˜\n",
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YpR6zG26VRYd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title æ§åˆ¶æ˜¾å­˜ç¢ç‰‡é…ç½®\n",
        "import os, sys\n",
        "\n",
        "# å¿…é¡»åœ¨ import torch / diffusers / transformers ä¹‹å‰è¿è¡Œ\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
        "\n",
        "# é˜²å‘†ï¼šå¦‚æœ torch å·²ç»è¢«å¯¼å…¥äº†ï¼Œå°±è¯´æ˜è¿™æ ¼è·‘æ™šäº†ï¼ˆæœ¬æ¬¡ä¸ä¼šç”Ÿæ•ˆï¼‰\n",
        "if \"torch\" in sys.modules:\n",
        "    raise RuntimeError(\"torch å·²å¯¼å…¥ï¼šallocator é…ç½®æœ¬æ¬¡ä¸ä¼šç”Ÿæ•ˆã€‚è¯·é‡å¯ runtime åç¬¬ä¸€æ ¼å…ˆè·‘è¿™ä¸ª cellã€‚\")\n",
        "\n",
        "print(\"âœ… PYTORCH_CUDA_ALLOC_CONF =\", os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])"
      ],
      "metadata": {
        "id": "NZ1RC_asHd9Z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ç¯å¢ƒé…ç½®\n",
        "%%capture\n",
        "!pip install -U --prefer-binary -q \\\n",
        "  git+https://github.com/Disty0/sdnq \\\n",
        "  git+https://github.com/huggingface/diffusers\n",
        "\n",
        "!pip install -U -q transformers accelerate"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GS7Xeo6-1Yya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ”¥ å®‰è£… cache-ditï¼ˆå«é‡åŒ–ä¸ç¼“å­˜åŠ é€Ÿæ”¯æŒï¼‰\n",
        "%%capture\n",
        "!pip install -U -q \"cache-dit[quantization]\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "cache_dit_install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ¤– æ¨¡å‹ç®¡ç†å™¨ï¼ˆæ··æ­ç‰ˆï¼‰\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# ----------------------------\n",
        "# âœ… TF32 ç²¾åº¦æ§åˆ¶ (PyTorch æ–° API)\n",
        "# ----------------------------\n",
        "try:\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    torch.backends.cudnn.conv.fp32_precision = 'tf32'\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# é…ç½®ï¼šä¸¤å¥—æ¨¡å‹\n",
        "# ----------------------------\n",
        "MODEL_CATALOG = {\n",
        "    \"âš¡ uint4 (æ›´å¿«/é¢„è§ˆ)\": {\n",
        "        \"repo_id\": \"Disty0/Z-Image-Turbo-SDNQ-uint4-svd-r32\",\n",
        "        \"default_local_path\": \"/content/Z-Image-Turbo-SDNQ-uint4\",\n",
        "    },\n",
        "    \"ğŸ¯ int8 (æ›´å¥½/äº§å“)\": {\n",
        "        \"repo_id\": \"Disty0/Z-Image-Turbo-SDNQ-int8\",\n",
        "        \"default_local_path\": \"/content/Z-Image-Turbo-SDNQ-int8\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# å¿…é¡»ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆä¸¤å¥—æ¨¡å‹ä¸€è‡´ï¼‰\n",
        "REQUIRED_FILES = [\n",
        "    \"model_index.json\",\n",
        "    \"scheduler/scheduler_config.json\",\n",
        "    \"text_encoder/config.json\",\n",
        "    \"text_encoder/generation_config.json\",\n",
        "    \"text_encoder/model.safetensors\",\n",
        "    \"text_encoder/quantization_config.json\",\n",
        "    \"tokenizer/merges.txt\",\n",
        "    \"tokenizer/tokenizer.json\",\n",
        "    \"tokenizer/tokenizer_config.json\",\n",
        "    \"tokenizer/vocab.json\",\n",
        "    \"transformer/config.json\",\n",
        "    \"transformer/diffusion_pytorch_model.safetensors\",\n",
        "    \"transformer/quantization_config.json\",\n",
        "    \"vae/config.json\",\n",
        "    \"vae/diffusion_pytorch_model.safetensors\",\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# UI ç»„ä»¶\n",
        "# ----------------------------\n",
        "output = widgets.Output()\n",
        "\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=list(MODEL_CATALOG.keys()),\n",
        "    value=\"âš¡ uint4 (æ›´å¿«/é¢„è§ˆ)\",\n",
        "    description=\"é€‰æ‹©æ¨¡å‹:\",\n",
        "    layout=widgets.Layout(width=\"420px\"),\n",
        ")\n",
        "\n",
        "repo_html = widgets.HTML(value=\"\")\n",
        "\n",
        "path_input = widgets.Text(\n",
        "    value=MODEL_CATALOG[model_dropdown.value][\"default_local_path\"],\n",
        "    placeholder=\"æœ¬åœ°æ¨¡å‹ä¿å­˜/åŠ è½½è·¯å¾„\",\n",
        "    description=\"æ¨¡å‹è·¯å¾„:\",\n",
        "    layout=widgets.Layout(width=\"98%\")\n",
        ")\n",
        "\n",
        "# âœ… æ–°å¢ï¼šå¯é€‰å¤–ç½® VAE è·¯å¾„\n",
        "vae_path_input = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"ï¼ˆå¯é€‰ï¼‰ç•™ç©º=ç”¨æ¨¡å‹è‡ªå¸¦VAEï¼›ä¸ç©º=åŠ è½½è¯¥è·¯å¾„VAE(å¯å¡«æ¨¡å‹æ ¹æˆ–vaeå­ç›®å½•)\",\n",
        "    description=\"VAEè·¯å¾„:\",\n",
        "    layout=widgets.Layout(width=\"98%\")\n",
        ")\n",
        "vae_status_html = widgets.HTML(value=\"<span style='color:gray;'>VAEï¼šé»˜è®¤ï¼ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦ï¼‰</span>\")\n",
        "\n",
        "mode_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"ğŸŒ ä» HuggingFace åœ¨çº¿åŠ è½½\", \"online\"),\n",
        "        (\"ğŸ“ ä»æœ¬åœ°è·¯å¾„åŠ è½½\", \"local\"),\n",
        "        (\"â¬‡ï¸ ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°\", \"download\"),\n",
        "        (\"ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤æœ¬åœ°æ¨¡å‹\", \"repair\"),\n",
        "    ],\n",
        "    value=\"local\",\n",
        "    description=\"æ“ä½œæ¨¡å¼:\",\n",
        "    layout=widgets.Layout(width=\"350px\")\n",
        ")\n",
        "\n",
        "execute_btn = widgets.Button(\n",
        "    description=\"ğŸš€ æ‰§è¡Œ\",\n",
        "    button_style=\"primary\",\n",
        "    layout=widgets.Layout(width=\"120px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "cleanup_btn = widgets.Button(\n",
        "    description=\"ğŸ§¹ ä¸€é”®æ¸…é™¤æ—§æ¨¡å‹\",\n",
        "    button_style=\"warning\",\n",
        "    layout=widgets.Layout(width=\"160px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "status_html = widgets.HTML(value=\"<span style='color:gray;'>å‡†å¤‡å°±ç»ª</span>\")\n",
        "progress_html = widgets.HTML(value=\"\")\n",
        "\n",
        "local_status_html = widgets.HTML(value=\"\")\n",
        "\n",
        "# ----------------------------\n",
        "# å°å·¥å…·\n",
        "# ----------------------------\n",
        "def current_model_cfg():\n",
        "    return MODEL_CATALOG[model_dropdown.value]\n",
        "\n",
        "def update_repo_info():\n",
        "    cfg = current_model_cfg()\n",
        "    repo_html.value = f\"<div style='font-size:12px;color:#666;'><b>Repo:</b> {cfg['repo_id']}</div>\"\n",
        "\n",
        "def check_local_model(path):\n",
        "    if not os.path.exists(path):\n",
        "        return False, \"è·¯å¾„ä¸å­˜åœ¨\", []\n",
        "    missing_files = []\n",
        "    for f in REQUIRED_FILES:\n",
        "        fp = os.path.join(path, f)\n",
        "        if not os.path.exists(fp) or os.path.getsize(fp) == 0:\n",
        "            missing_files.append(f)\n",
        "    if missing_files:\n",
        "        return False, f\"ç¼ºå¤± {len(missing_files)} ä¸ªæ–‡ä»¶\", missing_files\n",
        "    return True, \"æ¨¡å‹å®Œæ•´\", []\n",
        "\n",
        "def refresh_local_status():\n",
        "    p = path_input.value.strip()\n",
        "    is_valid, msg, missing = check_local_model(p)\n",
        "    if is_valid:\n",
        "        local_status_html.value = \"<span style='color:green;'>âœ… æœ¬åœ°æ¨¡å‹å®Œæ•´ï¼Œå¯ç›´æ¥åŠ è½½</span>\"\n",
        "    elif os.path.exists(p):\n",
        "        local_status_html.value = f\"<span style='color:orange;'>âš ï¸ æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼š{msg}</span>\"\n",
        "    else:\n",
        "        local_status_html.value = \"<span style='color:gray;'>ğŸ“­ æœ¬åœ°æ— æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½</span>\"\n",
        "\n",
        "def _resolve_vae_dir(user_path: str) -> str:\n",
        "    \"\"\"\n",
        "    å…è®¸ç”¨æˆ·å¡«ï¼š\n",
        "    - ç›´æ¥ vae ç›®å½•\n",
        "    - æˆ–æ¨¡å‹æ ¹ç›®å½•ï¼ˆé‡Œé¢æœ‰ vae/ï¼‰\n",
        "    è¿”å›çœŸæ­£çš„ vae ç›®å½•è·¯å¾„\n",
        "    \"\"\"\n",
        "    p = (user_path or \"\").strip()\n",
        "    if not p:\n",
        "        return \"\"\n",
        "\n",
        "    if not os.path.exists(p):\n",
        "        raise RuntimeError(f\"VAE è·¯å¾„ä¸å­˜åœ¨: {p}\")\n",
        "\n",
        "    # 1) ç›´æ¥å°±æ˜¯ vae ç›®å½•\n",
        "    if os.path.isdir(p) and os.path.exists(os.path.join(p, \"config.json\")):\n",
        "        return p\n",
        "\n",
        "    # 2) æ˜¯æ¨¡å‹æ ¹ç›®å½•ï¼Œå°è¯• root/vae\n",
        "    cand = os.path.join(p, \"vae\")\n",
        "    if os.path.isdir(cand) and os.path.exists(os.path.join(cand, \"config.json\")):\n",
        "        return cand\n",
        "\n",
        "    # 3) å…œåº•ï¼šèµ°ä¸€å±‚æ‰«æï¼ˆé˜²æ­¢ç”¨æˆ·ç»™åˆ°å¥‡æ€ªç»“æ„ï¼‰\n",
        "    for dirpath, _, filenames in os.walk(p):\n",
        "        if \"config.json\" in filenames:\n",
        "            # æƒé‡æ–‡ä»¶åæŒ‰ diffusers å¸¸è§å‘½å\n",
        "            if (\"diffusion_pytorch_model.safetensors\" in filenames) or (\"diffusion_pytorch_model.bin\" in filenames):\n",
        "                if os.path.basename(dirpath).lower() == \"vae\":\n",
        "                    return dirpath\n",
        "\n",
        "    raise RuntimeError(\n",
        "        f\"åœ¨è¯¥è·¯å¾„ä¸‹æ‰¾ä¸åˆ°å¯ç”¨çš„ VAE ç›®å½•ï¼ˆéœ€è¦ config.json + diffusion_pytorch_model.safetensors/.binï¼‰: {p}\"\n",
        "    )\n",
        "\n",
        "def refresh_vae_status():\n",
        "    p = (vae_path_input.value or \"\").strip()\n",
        "    if not p:\n",
        "        vae_status_html.value = \"<span style='color:gray;'>VAEï¼šé»˜è®¤ï¼ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦ï¼‰</span>\"\n",
        "        return\n",
        "    try:\n",
        "        vd = _resolve_vae_dir(p)\n",
        "        vae_status_html.value = f\"<span style='color:green;'>âœ… VAEï¼šå°†ä½¿ç”¨å¤–ç½®è·¯å¾„</span><br><span style='color:#666;font-size:12px;'>resolved: {vd}</span>\"\n",
        "    except Exception as e:\n",
        "        vae_status_html.value = f\"<span style='color:red;'>âŒ VAE è·¯å¾„æ— æ•ˆï¼š{e}</span>\"\n",
        "\n",
        "def on_model_change(change):\n",
        "    cfg = current_model_cfg()\n",
        "    path_input.value = cfg[\"default_local_path\"]\n",
        "    update_repo_info()\n",
        "    refresh_local_status()\n",
        "\n",
        "model_dropdown.observe(on_model_change, names=\"value\")\n",
        "update_repo_info()\n",
        "\n",
        "vae_path_input.observe(lambda c: refresh_vae_status(), names=\"value\")\n",
        "refresh_vae_status()\n",
        "\n",
        "# ----------------------------\n",
        "# ä¸‹è½½/ä¿®å¤\n",
        "# ----------------------------\n",
        "def download_file(repo_id, filename, save_path):\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    try:\n",
        "        hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=filename,\n",
        "            local_dir=save_path,\n",
        "            local_dir_use_symlinks=False,\n",
        "            force_download=True,\n",
        "        )\n",
        "        local_file = os.path.join(save_path, filename)\n",
        "        if os.path.exists(local_file) and os.path.getsize(local_file) > 0:\n",
        "            return True, os.path.getsize(local_file)\n",
        "        return False, 0\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "def download_all_files(repo_id, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    results = {\"success\": 0, \"failed\": 0, \"skipped\": 0}\n",
        "    failed_files = []\n",
        "\n",
        "    for i, filename in enumerate(REQUIRED_FILES):\n",
        "        local_file = os.path.join(save_path, filename)\n",
        "        progress_html.value = f\"<span style='color:blue;'>ğŸ“¥ [{i+1}/{len(REQUIRED_FILES)}] {filename}</span>\"\n",
        "\n",
        "        if os.path.exists(local_file) and os.path.getsize(local_file) > 0:\n",
        "            size_mb = os.path.getsize(local_file) / (1024 * 1024)\n",
        "            print(f\"â­ï¸ å·²å­˜åœ¨: {filename} ({size_mb:.2f} MB)\")\n",
        "            results[\"skipped\"] += 1\n",
        "            continue\n",
        "\n",
        "        print(f\"â¬‡ï¸ ä¸‹è½½: {filename}...\")\n",
        "        success, info = download_file(repo_id, filename, save_path)\n",
        "        if success:\n",
        "            print(f\"   âœ… å®Œæˆ ({info/(1024*1024):.2f} MB)\")\n",
        "            results[\"success\"] += 1\n",
        "        else:\n",
        "            print(f\"   âŒ å¤±è´¥: {info}\")\n",
        "            results[\"failed\"] += 1\n",
        "            failed_files.append(filename)\n",
        "\n",
        "    progress_html.value = \"\"\n",
        "    return results, failed_files\n",
        "\n",
        "def repair_model(repo_id, save_path):\n",
        "    is_valid, msg, missing = check_local_model(save_path)\n",
        "    if is_valid:\n",
        "        print(\"âœ… æ¨¡å‹å·²å®Œæ•´ï¼Œæ— éœ€ä¿®å¤\")\n",
        "        return True\n",
        "\n",
        "    print(f\"ğŸ”§ å‘ç° {len(missing)} ä¸ªç¼ºå¤±æ–‡ä»¶ï¼Œå¼€å§‹ä¿®å¤...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    ok = 0\n",
        "    for i, filename in enumerate(missing):\n",
        "        progress_html.value = f\"<span style='color:orange;'>ğŸ”§ [{i+1}/{len(missing)}] {filename}</span>\"\n",
        "        print(f\"â¬‡ï¸ ä¸‹è½½: {filename}...\")\n",
        "        success, info = download_file(repo_id, filename, save_path)\n",
        "        if success:\n",
        "            print(f\"   âœ… å®Œæˆ ({info/(1024*1024):.2f} MB)\")\n",
        "            ok += 1\n",
        "        else:\n",
        "            print(f\"   âŒ å¤±è´¥: {info}\")\n",
        "\n",
        "    progress_html.value = \"\"\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"ğŸ“Š ä¿®å¤å®Œæˆ: {ok}/{len(missing)}\")\n",
        "\n",
        "    is_valid, _, _ = check_local_model(save_path)\n",
        "    return is_valid\n",
        "\n",
        "# ----------------------------\n",
        "# ğŸ§¹ ä¸€é”®æ¸…é™¤ï¼ˆé‡Šæ”¾æ—§pipe/ç¼“å­˜ï¼‰\n",
        "# ----------------------------\n",
        "def hard_cleanup(drop_loaded_components: bool = True):\n",
        "    global pipe\n",
        "    freed = []\n",
        "\n",
        "    if \"pipe\" in globals() and pipe is not None:\n",
        "        try:\n",
        "            del pipe\n",
        "            freed.append(\"pipe\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        finally:\n",
        "            pipe = None\n",
        "\n",
        "    if drop_loaded_components and \"loaded_components\" in globals():\n",
        "        try:\n",
        "            del globals()[\"loaded_components\"]\n",
        "            freed.append(\"loaded_components\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            torch.cuda.ipc_collect()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        gc.collect()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return freed\n",
        "\n",
        "def on_cleanup_click(b):\n",
        "    output.clear_output()\n",
        "    cleanup_btn.disabled = True\n",
        "    with output:\n",
        "        try:\n",
        "            status_html.value = \"<span style='color:orange;'>ğŸ§¹ æ¸…ç†ä¸­...</span>\"\n",
        "            freed = hard_cleanup(drop_loaded_components=True)\n",
        "            print(\"ğŸ§¹ æ¸…ç†å®Œæˆ\")\n",
        "            print(\"é‡Šæ”¾å¯¹è±¡:\", \", \".join(freed) if freed else \"(æ— å¯é‡Šæ”¾å¯¹è±¡ / å·²æ˜¯å¹²å‡€çŠ¶æ€)\")\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "                reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "                print(f\"ğŸ’¾ æ˜¾å­˜: {allocated:.2f} GB (ä½¿ç”¨) / {reserved:.2f} GB (ä¿ç•™)\")\n",
        "\n",
        "            status_html.value = \"<span style='color:green;'>âœ… å·²æ¸…ç†</span>\"\n",
        "        except Exception as e:\n",
        "            print(\"âŒ æ¸…ç†å¤±è´¥:\", e)\n",
        "            status_html.value = \"<span style='color:red;'>âŒ æ¸…ç†å¤±è´¥</span>\"\n",
        "        finally:\n",
        "            cleanup_btn.disabled = False\n",
        "\n",
        "cleanup_btn.on_click(on_cleanup_click)\n",
        "\n",
        "# ----------------------------\n",
        "# TE æŒ‰éœ€åŠ è½½/å¸è½½ï¼ˆä¾›â€œç”Ÿlatentå•å…ƒâ€è°ƒç”¨ï¼‰\n",
        "# ----------------------------\n",
        "def set_pipe_model_root(pipe, model_path: str):\n",
        "    pipe._model_root = model_path\n",
        "\n",
        "def drop_text_encoder(pipe, drop_tokenizer: bool = False):\n",
        "    try:\n",
        "        te = getattr(pipe, \"text_encoder\", None)\n",
        "        pipe.text_encoder = None\n",
        "        if te is not None:\n",
        "            del te\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if drop_tokenizer:\n",
        "        try:\n",
        "            tok = getattr(pipe, \"tokenizer\", None)\n",
        "            pipe.tokenizer = None\n",
        "            if tok is not None:\n",
        "                del tok\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def ensure_text_encoder_loaded(pipe, device: str = \"cpu\", dtype=torch.float32):\n",
        "    if getattr(pipe, \"text_encoder\", None) is not None:\n",
        "        return pipe.text_encoder\n",
        "\n",
        "    root = getattr(pipe, \"_model_root\", None)\n",
        "    if root is None:\n",
        "        raise RuntimeError(\"pipe._model_root æœªè®¾ç½®ï¼Œæ— æ³•æŒ‰éœ€åŠ è½½ text_encoder\")\n",
        "\n",
        "    te_dir = os.path.join(root, \"text_encoder\")\n",
        "    if not os.path.exists(te_dir):\n",
        "        raise RuntimeError(\n",
        "            f\"æ‰¾ä¸åˆ°æœ¬åœ° text_encoder ç›®å½•: {te_dir}\"\n",
        "            f\"æç¤ºï¼šæŒ‰éœ€TEå»ºè®®ä½¿ç”¨â€œæœ¬åœ°åŠ è½½æ¨¡å¼â€ï¼Œæˆ–å…ˆæŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ã€‚\"\n",
        "        )\n",
        "\n",
        "    from transformers import AutoModel\n",
        "    from sdnq.loader import apply_sdnq_options_to_model\n",
        "\n",
        "    te = AutoModel.from_pretrained(te_dir, torch_dtype=dtype, local_files_only=True)\n",
        "    te = apply_sdnq_options_to_model(te, use_quantized_matmul=True)\n",
        "\n",
        "    try:\n",
        "        te.to(device)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    pipe.text_encoder = te\n",
        "    return te\n",
        "\n",
        "def ensure_tokenizer_loaded(pipe):\n",
        "    if getattr(pipe, \"tokenizer\", None) is not None:\n",
        "        return pipe.tokenizer\n",
        "\n",
        "    root = getattr(pipe, \"_model_root\", None)\n",
        "    if root is None:\n",
        "        raise RuntimeError(\"pipe._model_root æœªè®¾ç½®ï¼Œæ— æ³•æŒ‰éœ€åŠ è½½ tokenizer\")\n",
        "\n",
        "    tok_dir = os.path.join(root, \"tokenizer\")\n",
        "    if not os.path.exists(tok_dir):\n",
        "        raise RuntimeError(\n",
        "            f\"æ‰¾ä¸åˆ°æœ¬åœ° tokenizer ç›®å½•: {tok_dir}\"\n",
        "            f\"æç¤ºï¼šæŒ‰éœ€ tokenizer å»ºè®®ä½¿ç”¨â€œæœ¬åœ°åŠ è½½æ¨¡å¼â€ï¼Œæˆ–å…ˆæŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ã€‚\"\n",
        "        )\n",
        "\n",
        "    from transformers import AutoTokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(tok_dir, local_files_only=True)\n",
        "    pipe.tokenizer = tok\n",
        "    return tok\n",
        "\n",
        "# ----------------------------\n",
        "# âœ… æ–°å¢ï¼šå¤–ç½® VAE åŠ è½½å¹¶è¦†ç›– pipe.vae\n",
        "# ----------------------------\n",
        "def maybe_override_vae(pipe, vae_path_text: str, local_only_hint: bool):\n",
        "    \"\"\"\n",
        "    vae_path_text ä¸ºç©ºï¼šä¸æ”¹åŠ¨ï¼ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦ VAEï¼‰\n",
        "    vae_path_text ä¸ä¸ºç©ºï¼šä»è¯¥è·¯å¾„åŠ è½½ VAE å¹¶è¦†ç›– pipe.vae\n",
        "    local_only_hintï¼šå½“å‰åŠ è½½æ¨¡å¼æ˜¯å¦ local_onlyï¼ˆåœ¨çº¿æ¨¡å¼ä¹Ÿå…è®¸å¤–ç½®VAE=æœ¬åœ°ï¼‰\n",
        "    \"\"\"\n",
        "    vae_path_text = (vae_path_text or \"\").strip()\n",
        "    if not vae_path_text:\n",
        "        return False, \"VAE: é»˜è®¤ï¼ˆä½¿ç”¨æ¨¡å‹è‡ªå¸¦ï¼‰\"\n",
        "\n",
        "    vae_dir = _resolve_vae_dir(vae_path_text)\n",
        "\n",
        "    from diffusers import AutoencoderKL\n",
        "\n",
        "    # å¤–ç½®VAEä¸€èˆ¬æ˜¯æœ¬åœ°ç›®å½•ï¼Œæ‰€ä»¥è¿™é‡Œå¼ºåˆ¶ local_files_only=True æ›´ç¨³\n",
        "    # dtypeï¼šä½ å½“å‰ pipeline ç”¨ FP32 åŸºçº¿ï¼›ä½† VAE é€šå¸¸ FP16 è§£ç æ›´çœæ˜¾å­˜\n",
        "    # è¿™é‡ŒåŠ è½½ä¸º float16ï¼Œä¸å½±å“ latent å…¼å®¹æ€§ï¼ˆåªæ˜¯è§£ç é˜¶æ®µï¼‰\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        vae_dir,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        local_files_only=True,\n",
        "    )\n",
        "\n",
        "    # è¦†ç›–æ—§ VAE\n",
        "    old = getattr(pipe, \"vae\", None)\n",
        "    pipe.vae = vae\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            pipe.vae.to(\"cuda\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        if old is not None:\n",
        "            del old\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return True, f\"VAE: å·²è¦†ç›–ä¸ºå¤–ç½®è·¯å¾„ -> {vae_dir}\"\n",
        "\n",
        "# ----------------------------\n",
        "# ğŸ”¥ cache-dit åˆå§‹åŒ–\n",
        "# ----------------------------\n",
        "# æ›¿æ¢åŸæœ‰çš„ try_enable_cache_dit å‡½æ•°\n",
        "def try_enable_cache_dit(pipe):\n",
        "    try:\n",
        "        import cache_dit\n",
        "        from cache_dit import ForwardPattern, BlockAdapter, DBCacheConfig\n",
        "\n",
        "        # T4 16GBä¿å®ˆé…ç½®\n",
        "        cache_config = DBCacheConfig(\n",
        "            Fn_compute_blocks=1,\n",
        "            residual_diff_threshold=0.3,  # æ›´é«˜é˜ˆå€¼ï¼Œæ›´æ¿€è¿›ç¼“å­˜\n",
        "            max_warmup_steps=1,\n",
        "            max_cached_steps=25,\n",
        "        )\n",
        "\n",
        "        cache_dit.enable_cache(\n",
        "            BlockAdapter(pipe=pipe, auto=True, forward_pattern=ForwardPattern.Pattern_1),\n",
        "            cache_config=cache_config\n",
        "        )\n",
        "        print(\"âœ… cache-dit: T4-optimized config enabled\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ cache-dit failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# ----------------------------\n",
        "# åŠ è½½æ¨¡å‹ï¼šå›ºå®šä¸å¸¸é©»TEï¼ˆä¿æŒå…¶ä½™åŠ è½½é€»è¾‘/å‚æ•°ä¸å˜ï¼‰\n",
        "# ----------------------------\n",
        "def load_model(model_id_or_path, local_only=False, keep_text_encoder=False):\n",
        "    global pipe\n",
        "\n",
        "    import diffusers\n",
        "    from sdnq.loader import apply_sdnq_options_to_model\n",
        "\n",
        "    load_kwargs = {\n",
        "        \"torch_dtype\": torch.float32,\n",
        "        \"device_map\": \"cuda\",\n",
        "    }\n",
        "    if local_only:\n",
        "        load_kwargs[\"local_files_only\"] = True\n",
        "\n",
        "    pipe = diffusers.ZImagePipeline.from_pretrained(model_id_or_path, **load_kwargs)\n",
        "    set_pipe_model_root(pipe, model_id_or_path)\n",
        "\n",
        "    pipe.transformer = apply_sdnq_options_to_model(pipe.transformer, use_quantized_matmul=True)\n",
        "\n",
        "    # âœ… ä¸å¸¸é©»TE\n",
        "    drop_text_encoder(pipe, drop_tokenizer=False)\n",
        "\n",
        "    # ğŸ”¥ å…³é”®ï¼šå¯ç”¨ cache-dit\n",
        "    try_enable_cache_dit(pipe)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# ----------------------------\n",
        "# UI å›è°ƒ\n",
        "# ----------------------------\n",
        "def on_execute_click(b):\n",
        "    global pipe\n",
        "\n",
        "    output.clear_output()\n",
        "    execute_btn.disabled = True\n",
        "    mode = mode_dropdown.value\n",
        "\n",
        "    keep_te = False  # å›ºå®šä¸å¸¸é©»TE\n",
        "\n",
        "    cfg = current_model_cfg()\n",
        "    repo_id = cfg[\"repo_id\"]\n",
        "    local_path = path_input.value.strip()\n",
        "    vae_path = vae_path_input.value.strip()\n",
        "\n",
        "    with output:\n",
        "        try:\n",
        "            if mode == \"download\":\n",
        "                print(f\"â¬‡ï¸ å¼€å§‹ä¸‹è½½æ¨¡å‹\\nğŸ“¦ æ¨¡å‹: {repo_id}\\nğŸ’¾ ä¿å­˜åˆ°: {local_path}\\n\" + \"=\" * 50)\n",
        "                status_html.value = \"<span style='color:orange;'>â³ ä¸‹è½½ä¸­...</span>\"\n",
        "\n",
        "                results, failed = download_all_files(repo_id, local_path)\n",
        "\n",
        "                print(\"=\" * 50)\n",
        "                print(f\"ğŸ“Š ç»“æœ: ä¸‹è½½ {results['success']}, è·³è¿‡ {results['skipped']}, å¤±è´¥ {results['failed']}\")\n",
        "                if results[\"failed\"] > 0:\n",
        "                    print(\"\\nâŒ å¤±è´¥çš„æ–‡ä»¶:\")\n",
        "                    for f in failed:\n",
        "                        print(f\"   - {f}\")\n",
        "                    status_html.value = f\"<span style='color:orange;'>âš ï¸ {results['failed']} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥</span>\"\n",
        "                else:\n",
        "                    total_size = sum(\n",
        "                        os.path.getsize(os.path.join(local_path, f))\n",
        "                        for f in REQUIRED_FILES if os.path.exists(os.path.join(local_path, f))\n",
        "                    )\n",
        "                    print(f\"\\nâœ… ä¸‹è½½å®Œæˆ! æ€»å¤§å°: {total_size / (1024**3):.2f} GB\")\n",
        "                    status_html.value = f\"<span style='color:green;'>âœ… ä¸‹è½½å®Œæˆ ({total_size / (1024**3):.2f} GB)</span>\"\n",
        "\n",
        "            elif mode == \"repair\":\n",
        "                print(f\"ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤æ¨¡å‹\\nğŸ“¦ æ¨¡å‹: {repo_id}\\nğŸ“‚ è·¯å¾„: {local_path}\\n\" + \"=\" * 50)\n",
        "                status_html.value = \"<span style='color:orange;'>â³ æ£€æŸ¥ä¸­...</span>\"\n",
        "\n",
        "                is_valid, msg, missing = check_local_model(local_path)\n",
        "                print(f\"å½“å‰çŠ¶æ€: {msg}\")\n",
        "                if missing:\n",
        "                    print(\"\\nç¼ºå¤±çš„æ–‡ä»¶:\")\n",
        "                    for f in missing:\n",
        "                        print(f\"   âŒ {f}\")\n",
        "                    print()\n",
        "\n",
        "                if repair_model(repo_id, local_path):\n",
        "                    status_html.value = \"<span style='color:green;'>âœ… æ¨¡å‹å·²å®Œæ•´</span>\"\n",
        "                else:\n",
        "                    status_html.value = \"<span style='color:red;'>âŒ ä»æœ‰æ–‡ä»¶ç¼ºå¤±</span>\"\n",
        "\n",
        "            elif mode == \"local\":\n",
        "                print(f\"ğŸ“ ä»æœ¬åœ°åŠ è½½æ¨¡å‹\\nğŸ“¦ æ¨¡å‹: {repo_id}\\nğŸ“‚ è·¯å¾„: {local_path}\\n\" + \"-\" * 50)\n",
        "                is_valid, msg, missing = check_local_model(local_path)\n",
        "                print(f\"æ¨¡å‹çŠ¶æ€: {msg}\")\n",
        "                if not is_valid:\n",
        "                    print(\"\\nâŒ æ¨¡å‹ä¸å®Œæ•´! ç¼ºå¤±æ–‡ä»¶:\")\n",
        "                    for f in missing:\n",
        "                        print(f\"   - {f}\")\n",
        "                    print(\"\\nğŸ’¡ å…ˆç”¨ã€Œä¿®å¤ã€æˆ–ã€Œä¸‹è½½ã€\")\n",
        "                    status_html.value = \"<span style='color:red;'>âŒ æ¨¡å‹ä¸å®Œæ•´</span>\"\n",
        "                    return\n",
        "\n",
        "                status_html.value = \"<span style='color:orange;'>â³ åŠ è½½ä¸­...</span>\"\n",
        "\n",
        "                if \"pipe\" in globals() and pipe is not None:\n",
        "                    del pipe\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                pipe = load_model(local_path, local_only=True, keep_text_encoder=keep_te)\n",
        "\n",
        "                # âœ… å…³é”®ï¼šæ ¹æ®è¾“å…¥æ¡†å†³å®šæ˜¯å¦è¦†ç›–VAE\n",
        "                changed, vae_msg = maybe_override_vae(pipe, vae_path, local_only_hint=True)\n",
        "                print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ!\")\n",
        "                print(f\"ğŸ§  TE å¸¸é©»: {keep_te}\")\n",
        "                print(f\"ğŸ“Œ å½“å‰æ¨¡å‹: {repo_id}\")\n",
        "                print(f\"ğŸ§© {vae_msg}\")\n",
        "                status_html.value = \"<span style='color:green;'>âœ… æ¨¡å‹å·²åŠ è½½ (æœ¬åœ°)</span>\"\n",
        "\n",
        "            elif mode == \"online\":\n",
        "                print(f\"ğŸŒ ä» HuggingFace åœ¨çº¿åŠ è½½\\nğŸ“¦ æ¨¡å‹: {repo_id}\\n\" + \"-\" * 50)\n",
        "                status_html.value = \"<span style='color:orange;'>â³ åŠ è½½ä¸­ï¼ˆé¦–æ¬¡éœ€ä¸‹è½½ï¼‰...</span>\"\n",
        "\n",
        "                if \"pipe\" in globals() and pipe is not None:\n",
        "                    del pipe\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                pipe = load_model(repo_id, local_only=False, keep_text_encoder=keep_te)\n",
        "\n",
        "                # âœ… åœ¨çº¿ä¹Ÿå…è®¸è¦†ç›–VAEï¼ˆå‰ææ˜¯ä½ ç»™çš„VAEè·¯å¾„åœ¨æœ¬åœ°ï¼‰\n",
        "                changed, vae_msg = maybe_override_vae(pipe, vae_path, local_only_hint=False)\n",
        "\n",
        "                print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ!\")\n",
        "                print(f\"ğŸ§  TE å¸¸é©»: {keep_te}\")\n",
        "                print(f\"ğŸ“Œ å½“å‰æ¨¡å‹: {repo_id}\")\n",
        "                print(f\"ğŸ§© {vae_msg}\")\n",
        "\n",
        "                if not keep_te:\n",
        "                    print(\"âš ï¸ æç¤ºï¼šåœ¨çº¿åŠ è½½ + TEæŒ‰éœ€åŠ è½½ ç»„åˆå¯èƒ½æ— æ³•æ‰¾åˆ°æœ¬åœ° text_encoder/tokenizer ç›®å½•ã€‚\")\n",
        "                    print(\"   å»ºè®®ï¼šå…ˆä¸‹è½½åˆ°æœ¬åœ°ï¼Œå†ç”¨â€œæœ¬åœ°åŠ è½½ + TEæŒ‰éœ€â€æ¥è·‘ä¸¤é˜¶æ®µæµç¨‹ã€‚\")\n",
        "                status_html.value = \"<span style='color:green;'>âœ… æ¨¡å‹å·²åŠ è½½ (åœ¨çº¿)</span>\"\n",
        "\n",
        "            # æ˜¾å­˜æ˜¾ç¤º\n",
        "            if torch.cuda.is_available() and \"pipe\" in globals() and pipe is not None:\n",
        "                allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "                reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "                print(f\"\\nğŸ’¾ æ˜¾å­˜: {allocated:.2f} GB (ä½¿ç”¨) / {reserved:.2f} GB (ä¿ç•™)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ é”™è¯¯: {str(e)}\")\n",
        "            status_html.value = \"<span style='color:red;'>âŒ å¤±è´¥</span>\"\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            execute_btn.disabled = False\n",
        "            progress_html.value = \"\"\n",
        "            refresh_vae_status()\n",
        "\n",
        "execute_btn.on_click(on_execute_click)\n",
        "\n",
        "# æœ¬åœ°çŠ¶æ€è”åŠ¨\n",
        "path_input.observe(lambda c: refresh_local_status(), names=\"value\")\n",
        "refresh_local_status()\n",
        "\n",
        "# ----------------------------\n",
        "# UI å¸ƒå±€\n",
        "# ----------------------------\n",
        "header = widgets.HTML(value=\"\"\"\n",
        "<div style='background:linear-gradient(135deg,#11998e 0%,#38ef7d 100%);\n",
        "            color:white; padding:12px 15px; border-radius:8px; margin-bottom:10px;'>\n",
        "    <h3 style='margin:0;'>ğŸ¤– æ¨¡å‹ç®¡ç†å™¨</h3>\n",
        "    <span style='font-size:12px;'>Z-Image-Turbo-SDNQ | åŒæ¨¡å‹ï¼ˆuint4é¢„è§ˆ/int8äº§å“ï¼‰| TEæŒ‰éœ€åŠ è½½ + ä¸€é”®æ¸…é™¤ + å¤–ç½®VAEå¯é€‰</span>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "layout = widgets.VBox([\n",
        "    header,\n",
        "    widgets.HBox([model_dropdown]),\n",
        "    repo_html,\n",
        "    local_status_html,\n",
        "    widgets.HTML(value=\"<hr style='margin:10px 0;'>\"),\n",
        "    mode_dropdown,\n",
        "    path_input,\n",
        "    vae_path_input,\n",
        "    vae_status_html,\n",
        "    widgets.HBox([execute_btn, cleanup_btn, status_html]),\n",
        "    progress_html,\n",
        "    output\n",
        "], layout=widgets.Layout(\n",
        "    padding=\"10px\",\n",
        "    border=\"1px solid #ddd\",\n",
        "    border_radius=\"10px\",\n",
        "    width=\"760px\"\n",
        "))\n",
        "\n",
        "display(layout)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hohzpuXPtDgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ›ï¸ Z-Image Turbo å‡ºå›¾é¢æ¿\n",
        "import os, gc, time, json, numpy as np, torch, traceback, inspect\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "from google.colab import drive\n",
        "\n",
        "# ========= ä¾èµ–ï¼šæ¨¡å‹ç®¡ç†å™¨çš„ 3 ä¸ªå‡½æ•° =========\n",
        "def _require_manager_funcs():\n",
        "    needed = [\"ensure_tokenizer_loaded\", \"ensure_text_encoder_loaded\", \"drop_text_encoder\"]\n",
        "    missing = [n for n in needed if n not in globals()]\n",
        "    if missing:\n",
        "        raise RuntimeError(\"ç¼ºå°‘æ¨¡å‹ç®¡ç†å™¨å‡½æ•°ï¼ˆå…ˆè¿è¡Œæ¨¡å‹ç®¡ç†å™¨å¹¶åŠ è½½æ¨¡å‹ï¼‰: \" + \", \".join(missing))\n",
        "    return (globals()[\"ensure_tokenizer_loaded\"],\n",
        "            globals()[\"ensure_text_encoder_loaded\"],\n",
        "            globals()[\"drop_text_encoder\"])\n",
        "\n",
        "def _get_existing_pipe():\n",
        "    if \"pipe\" in globals() and globals().get(\"pipe\", None) is not None:\n",
        "        return globals()[\"pipe\"]\n",
        "    if \"loaded_components\" in globals() and isinstance(globals()[\"loaded_components\"], dict):\n",
        "        return globals()[\"loaded_components\"].get(\"pipe\", None)\n",
        "    return None\n",
        "\n",
        "def _round_to_16(x: int) -> int:\n",
        "    return int(round(x / 16) * 16)\n",
        "\n",
        "def _pipe_accepts_kw(p, key: str) -> bool:\n",
        "    try:\n",
        "        sig = inspect.signature(p.__call__)\n",
        "        return key in sig.parameters\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# ========= allocator/æ˜¾å­˜å°å·¥å…· =========\n",
        "def _get_allocator_conf():\n",
        "    return os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", \"\")\n",
        "\n",
        "def _allocator_status_text():\n",
        "    conf = _get_allocator_conf()\n",
        "    if not conf:\n",
        "        return (\"<span style='color:orange;'>âš ï¸ allocator: æœªè®¾ç½® PYTORCH_CUDA_ALLOC_CONF</span>\"\n",
        "                \"<br><span style='color:#64748b;font-size:12px;'>å»ºè®®é‡å¯ runtime åæœ€å…ˆè®¾ç½®ï¼š\"\n",
        "                \"expandable_segments:True,max_split_size_mb:128</span>\")\n",
        "    ok = (\"expandable_segments:True\" in conf)\n",
        "    if ok:\n",
        "        return (f\"<span style='color:green;'>âœ… allocator: {conf}</span>\"\n",
        "                \"<br><span style='color:#64748b;font-size:12px;'>å·²å¯ç”¨ expandable_segmentsï¼ˆæ›´æŠ—ç¢ç‰‡ï¼Œé€‚åˆå†²å¤§å›¾ï¼‰</span>\")\n",
        "    return (f\"<span style='color:orange;'>âš ï¸ allocator: {conf}</span>\"\n",
        "            \"<br><span style='color:#64748b;font-size:12px;'>å»ºè®®åŒ…å« expandable_segments:Trueï¼ˆéœ€è¦é‡å¯åç”Ÿæ•ˆï¼‰</span>\")\n",
        "\n",
        "def _print_cuda_mem_line(tag=\"\"):\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA ä¸å¯ç”¨\")\n",
        "        return\n",
        "    alloc = torch.cuda.memory_allocated() / (1024**3)\n",
        "    resv  = torch.cuda.memory_reserved() / (1024**3)\n",
        "    peak  = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    if tag:\n",
        "        print(f\"[{tag}] \", end=\"\")\n",
        "    print(f\"allocated={alloc:.2f} GB | reserved={resv:.2f} GB | peak={peak:.2f} GB\")\n",
        "\n",
        "def runtime_fragmentation_cleanup(synchronize=True, reset_peak=False):\n",
        "    \"\"\"è¿è¡Œæ—¶ç¢ç‰‡æ•´ç†ï¼šå¯¹å½“å‰è¿›ç¨‹æœ‰æ•ˆï¼ˆå³ä½¿ allocator æ²¡ç”Ÿæ•ˆä¹Ÿæœ‰æ”¶ç›Šï¼‰\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "    if synchronize:\n",
        "        try: torch.cuda.synchronize()\n",
        "        except Exception: pass\n",
        "    try: gc.collect()\n",
        "    except Exception: pass\n",
        "    try: torch.cuda.empty_cache()\n",
        "    except Exception: pass\n",
        "    try: torch.cuda.ipc_collect()\n",
        "    except Exception: pass\n",
        "    if reset_peak:\n",
        "        try: torch.cuda.reset_peak_memory_stats()\n",
        "        except Exception: pass\n",
        "    if synchronize:\n",
        "        try: torch.cuda.synchronize()\n",
        "        except Exception: pass\n",
        "\n",
        "# ========= SDPA ä¸Šä¸‹æ–‡ =========\n",
        "def _enter_sdpa_efficient():\n",
        "    try:\n",
        "        from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "        ctx = sdpa_kernel([SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH])\n",
        "        ctx.__enter__()\n",
        "        return ctx\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _exit_sdpa(ctx):\n",
        "    if ctx is None:\n",
        "        return\n",
        "    try:\n",
        "        ctx.__exit__(None, None, None)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ========= dtype / å°å·¥å…·ï¼ˆä¿ç•™ï¼šä¸åŠ¨åŸºçº¿ï¼‰ =========\n",
        "def _dtype_from_mode(mode: str):\n",
        "    mode = (mode or \"fp32\").lower()\n",
        "    if mode == \"fp16\":\n",
        "        return torch.float16\n",
        "    if mode == \"bf16\":\n",
        "        return torch.bfloat16\n",
        "    return torch.float32\n",
        "\n",
        "def _bf16_is_supported_cuda():\n",
        "    if not torch.cuda.is_available():\n",
        "        return False\n",
        "    try:\n",
        "        return bool(torch.cuda.is_bf16_supported())\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _maybe_cast_pipe_modules(p, target_dtype: torch.dtype, enable: bool):\n",
        "    if not enable:\n",
        "        return\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "    try:\n",
        "        if hasattr(p, \"transformer\") and p.transformer is not None:\n",
        "            p.transformer.to(\"cuda\", dtype=target_dtype)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        if hasattr(p, \"vae\") and p.vae is not None:\n",
        "            p.vae.to(\"cuda\", dtype=target_dtype)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ========= æ ¸å¿ƒæ¨ç†/è§£ç é€»è¾‘ =========\n",
        "def _prepare_full_gpu(p, attn_slice):\n",
        "    if hasattr(p, \"reset_device_map\"):\n",
        "        try: p.reset_device_map()\n",
        "        except Exception: pass\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"æ²¡æœ‰æ£€æµ‹åˆ° CUDA\")\n",
        "\n",
        "    runtime_fragmentation_cleanup(synchronize=True, reset_peak=False)\n",
        "\n",
        "    p.to(\"cuda\")\n",
        "    if hasattr(p, \"enable_attention_slicing\"):\n",
        "        if attn_slice == \"auto\":\n",
        "            attn_slice = \"max\"\n",
        "        p.enable_attention_slicing(attn_slice)\n",
        "\n",
        "def _apply_int8_matmul_to_transformer(p, enable: bool):\n",
        "    try:\n",
        "        from sdnq.loader import apply_sdnq_options_to_model\n",
        "    except Exception as e:\n",
        "        return False, f\"æœªèƒ½å¯¼å…¥ sdnq.loader: {e}\"\n",
        "    if not hasattr(p, \"transformer\") or p.transformer is None:\n",
        "        return False, \"pipe.transformer ä¸å­˜åœ¨\"\n",
        "    try:\n",
        "        p.transformer = apply_sdnq_options_to_model(p.transformer, use_quantized_matmul=bool(enable))\n",
        "        return True, f\"INT8 MatMul = {bool(enable)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"è®¾ç½®å¤±è´¥: {e}\"\n",
        "\n",
        "# ========= CFG>1 æ—¶å¿…é¡»æä¾› negative_prompt_embeds =========\n",
        "def _encode_prompt_embeds_then_drop_te(p, prompt_text: str, negative_text: str | None, cfg: float, te_dtype=torch.float32):\n",
        "    ensure_tokenizer_loaded_fn, ensure_text_encoder_loaded_fn, drop_text_encoder_fn = _require_manager_funcs()\n",
        "\n",
        "    if not _pipe_accepts_kw(p, \"prompt_embeds\"):\n",
        "        raise RuntimeError(\"pipe ä¸æ”¯æŒ prompt_embeds\")\n",
        "\n",
        "    ensure_tokenizer_loaded_fn(p)\n",
        "\n",
        "    te = ensure_text_encoder_loaded_fn(p, device=\"cuda\", dtype=torch.float32)\n",
        "    try:\n",
        "        te.to(\"cuda\", dtype=te_dtype)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    encode_fn = getattr(p, \"encode_prompt\", None) or getattr(p, \"_encode_prompt\", None)\n",
        "    if encode_fn is None:\n",
        "        raise RuntimeError(\"pipe æ²¡æœ‰ encode_prompt/_encode_prompt\")\n",
        "\n",
        "    sig = inspect.signature(encode_fn)\n",
        "    keys = set(sig.parameters.keys())\n",
        "\n",
        "    def _encode_one(text: str):\n",
        "        kwargs = {}\n",
        "        if \"prompt\" in keys:\n",
        "            kwargs[\"prompt\"] = text\n",
        "        if \"device\" in keys:\n",
        "            kwargs[\"device\"] = torch.device(\"cuda\")\n",
        "        if \"num_images_per_prompt\" in keys:\n",
        "            kwargs[\"num_images_per_prompt\"] = 1\n",
        "        if \"do_classifier_free_guidance\" in keys:\n",
        "            kwargs[\"do_classifier_free_guidance\"] = False\n",
        "        if \"negative_prompt\" in keys:\n",
        "            kwargs[\"negative_prompt\"] = None\n",
        "\n",
        "        encoded = encode_fn(**kwargs)\n",
        "\n",
        "        out = {}\n",
        "        if isinstance(encoded, torch.Tensor):\n",
        "            out[\"prompt_embeds\"] = encoded\n",
        "        elif isinstance(encoded, (tuple, list)):\n",
        "            out[\"prompt_embeds\"] = encoded[0]\n",
        "            if len(encoded) >= 3 and isinstance(encoded[2], torch.Tensor):\n",
        "                out[\"pooled_prompt_embeds\"] = encoded[2]\n",
        "        elif isinstance(encoded, dict):\n",
        "            for k in [\"prompt_embeds\", \"pooled_prompt_embeds\", \"attention_mask\"]:\n",
        "                if k in encoded:\n",
        "                    out[k] = encoded[k]\n",
        "        else:\n",
        "            raise RuntimeError(f\"æœªçŸ¥ encode è¿”å›ç±»å‹: {type(encoded)}\")\n",
        "        return out\n",
        "\n",
        "    do_cfg = bool(cfg is not None and float(cfg) > 1.0)\n",
        "\n",
        "    pos = _encode_one(prompt_text)\n",
        "    embeds_kwargs = {\"prompt_embeds\": pos[\"prompt_embeds\"]}\n",
        "    if \"pooled_prompt_embeds\" in pos:\n",
        "        embeds_kwargs[\"pooled_prompt_embeds\"] = pos[\"pooled_prompt_embeds\"]\n",
        "\n",
        "    if do_cfg:\n",
        "        neg_text = negative_text if (negative_text and negative_text.strip()) else \"\"\n",
        "        neg = _encode_one(neg_text)\n",
        "        embeds_kwargs[\"negative_prompt_embeds\"] = neg[\"prompt_embeds\"]\n",
        "        if \"pooled_prompt_embeds\" in neg:\n",
        "            embeds_kwargs[\"negative_pooled_prompt_embeds\"] = neg[\"pooled_prompt_embeds\"]\n",
        "\n",
        "    drop_text_encoder_fn(p, drop_tokenizer=False)\n",
        "\n",
        "    for k, v in list(embeds_kwargs.items()):\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            embeds_kwargs[k] = v.to(\"cuda\")\n",
        "    return embeds_kwargs\n",
        "\n",
        "def _decode_latent_with_pipe_vae(p, latent_t: torch.Tensor, scaling_factor: float, tiled: bool, tile_size):\n",
        "    if not hasattr(p, \"vae\") or p.vae is None:\n",
        "        raise RuntimeError(\"pipe.vae ä¸å­˜åœ¨\")\n",
        "    vae = p.vae\n",
        "\n",
        "    vae.to(\"cuda\", dtype=torch.float16)\n",
        "\n",
        "    lat = latent_t\n",
        "    if lat.dim() == 3:\n",
        "        lat = lat.unsqueeze(0)\n",
        "    lat = lat.to(\"cuda\", dtype=torch.float16)\n",
        "    with torch.no_grad():\n",
        "        if tiled and hasattr(vae, \"enable_tiling\"):\n",
        "            vae.enable_tiling()\n",
        "            try:\n",
        "                vae.tile_sample_size = int(tile_size)\n",
        "            except Exception:\n",
        "                pass\n",
        "        decoded = vae.decode(lat / float(scaling_factor)).sample\n",
        "    img = (decoded[0] / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).float().numpy()\n",
        "    return Image.fromarray((img * 255).astype(np.uint8))\n",
        "\n",
        "# ========= å›¾åƒæ¯”ä¾‹é¢„è®¾ =========\n",
        "ASPECT_RATIOS = {\n",
        "    \"è‡ªå®šä¹‰\": None,\n",
        "    \"1:1 æ–¹å½¢ (1024Ã—1024)\": (1024, 1024),\n",
        "    \"4:3 æ¨ªå± (1152Ã—864)\": (1152, 864),\n",
        "    \"3:4 ç«–å± (864Ã—1152)\": (864, 1152),\n",
        "    \"16:9 å®½å± (1344Ã—768)\": (1344, 768),\n",
        "    \"9:16 æ‰‹æœºå± (768Ã—1344)\": (768, 1344),\n",
        "    \"3:2 ç…§ç‰‡æ¨ª (1216Ã—816)\": (1216, 816),\n",
        "    \"2:3 ç…§ç‰‡ç«– (816Ã—1216)\": (816, 1216),\n",
        "    \"21:9 è¶…å®½ (1536Ã—656)\": (1536, 656),\n",
        "}\n",
        "\n",
        "def _auto_tile_for_size(w, h):\n",
        "    m = max(w, h)\n",
        "    if m >= 1536: return 512\n",
        "    if m >= 1024: return 768\n",
        "    return 1024\n",
        "\n",
        "# ========= Drive æŒ‚è½½ =========\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# ========= æ ·å¼å®šä¹‰ =========\n",
        "STYLE_CSS = \"\"\"\n",
        "<style>\n",
        ".zimg-panel { font-family: 'Segoe UI', sans-serif; }\n",
        ".zimg-section {\n",
        "    background: #1e293b; border-radius: 12px; padding: 16px; margin: 8px 0;\n",
        "    border: 1px solid #334155;\n",
        "}\n",
        ".zimg-section-title {\n",
        "    color: #38bdf8; font-size: 14px; font-weight: 600;\n",
        "    margin-bottom: 12px; padding-bottom: 8px;\n",
        "    border-bottom: 1px solid #334155;\n",
        "}\n",
        ".zimg-header {\n",
        "    background: linear-gradient(135deg, #0ea5e9 0%, #8b5cf6 50%, #ec4899 100%);\n",
        "    border-radius: 16px; padding: 20px; margin-bottom: 16px;\n",
        "    box-shadow: 0 4px 20px rgba(14, 165, 233, 0.3);\n",
        "}\n",
        ".zimg-header h1 { color: white; font-size: 24px; font-weight: 700; margin: 0 0 4px 0; }\n",
        ".zimg-header p { color: rgba(255,255,255,0.85); font-size: 12px; margin: 0; }\n",
        ".zimg-label { color: #94a3b8; font-size: 12px; margin-bottom: 4px; }\n",
        ".zimg-info { color: #64748b; font-size: 11px; margin-top: 4px; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# ========= UI ç»„ä»¶ =========\n",
        "header_html = widgets.HTML(STYLE_CSS + \"\"\"\n",
        "<div class=\"zimg-panel\">\n",
        "<div class=\"zimg-header\">\n",
        "    <h1>ğŸ¨ Z-Image Turbo</h1>\n",
        "    <p>TE ç¼–ç åå¸è½½ Â· VAE å¸¸é©»æ˜¾å­˜ Â· SDPA(Efficient) Â· TEå¸è½½åç¢ç‰‡æ•´ç† Â· å…ƒæ•°æ®å®Œæ•´ä¿å­˜</p>\n",
        "</div>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "allocator_html = widgets.HTML(value=f\"\"\"\n",
        "<div style=\"margin:10px 0 0 0; padding:10px 12px; border:1px solid #334155; border-radius:12px; background:#0b1220;\">\n",
        "  <div style=\"font-size:13px; color:#e2e8f0; font-weight:600; margin-bottom:4px;\">ğŸ§  Boot/Allocator çŠ¶æ€</div>\n",
        "  <div style=\"font-size:13px;\">{_allocator_status_text()}</div>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "# ==================== 1. æç¤ºè¯åŒºåŸŸ ====================\n",
        "prompt_box = widgets.Textarea(\n",
        "    value=\"a beautiful cat sitting on a windowsill, soft sunlight, detailed fur, photorealistic\",\n",
        "    placeholder=\"è¾“å…¥æ­£å‘æç¤ºè¯...\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"300px\"),\n",
        ")\n",
        "neg_box = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"è¾“å…¥è´Ÿé¢æç¤ºè¯ï¼ˆCFGâ‰¤1æ— æ•ˆï¼‰...\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"60px\"),\n",
        ")\n",
        "\n",
        "# ==================== âœ… æ–°å¢ï¼šä» PNG ä¸€é”®è¯»å–å…ƒæ•°æ®å¹¶å›å¡« ====================\n",
        "w_png_meta_path = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"å¡«å†™è¦è¯»å–å…ƒæ•°æ®çš„ PNG è·¯å¾„ï¼ˆä¾‹å¦‚ /content/drive/MyDrive/Z-image/Outputs/img_xxx.pngï¼‰\",\n",
        "    description=\"PNG è·¯å¾„\",\n",
        "    style={'description_width': '90px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "btn_load_meta = widgets.Button(\n",
        "    description=\"ğŸ“¥ ä¸€é”®è¯»å– PNG å…ƒæ•°æ®å¹¶å›å¡«å‚æ•°\",\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(width=\"320px\", height=\"36px\")\n",
        ")\n",
        "\n",
        "btn_copy_meta_json = widgets.Button(\n",
        "    description=\"ğŸ“‹ å¤åˆ¶å…ƒæ•°æ® JSON åˆ°å‰ªè´´æ¿\",\n",
        "    button_style=\"\",\n",
        "    layout=widgets.Layout(width=\"240px\", height=\"36px\")\n",
        ")\n",
        "\n",
        "meta_status = widgets.HTML(value=\"<div class='zimg-info'>åœ¨æ­¤å¡« PNG è·¯å¾„åï¼Œå¯ä¸€é”®è¯»å– parameters å…ƒæ•°æ®å¹¶å›å¡«é¢æ¿å‚æ•°ã€‚</div>\")\n",
        "\n",
        "def _read_metadata_from_png(png_path: str) -> dict | None:\n",
        "    \"\"\"ä» PNG æ–‡ä»¶è¯»å–å…ƒæ•°æ®ï¼ˆparameters JSONï¼‰ï¼Œä¸æœ¬ cell çš„ä¿å­˜æ ¼å¼åŒ¹é…\"\"\"\n",
        "    try:\n",
        "        img = Image.open(png_path)\n",
        "        if hasattr(img, \"info\") and \"parameters\" in img.info:\n",
        "            return json.loads(img.info[\"parameters\"])\n",
        "    except Exception as e:\n",
        "        print(f\"è¯»å– PNG å…ƒæ•°æ®å¤±è´¥: {e}\")\n",
        "    return None\n",
        "\n",
        "def _safe_float(v, default=None):\n",
        "    try:\n",
        "        if v is None: return default\n",
        "        return float(v)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _safe_int(v, default=None):\n",
        "    try:\n",
        "        if v is None: return default\n",
        "        return int(float(v))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _set_ratio_preset_if_exact_match(width: int, height: int) -> str:\n",
        "    for name, wh in ASPECT_RATIOS.items():\n",
        "        if wh is None:\n",
        "            continue\n",
        "        if int(wh[0]) == int(width) and int(wh[1]) == int(height):\n",
        "            return name\n",
        "    return \"è‡ªå®šä¹‰\"\n",
        "\n",
        "def _apply_metadata_to_ui(meta: dict):\n",
        "    # prompt / negative\n",
        "    if isinstance(meta.get(\"prompt\", None), str):\n",
        "        prompt_box.value = meta[\"prompt\"]\n",
        "    if isinstance(meta.get(\"negative_prompt\", None), str):\n",
        "        neg_box.value = meta[\"negative_prompt\"]\n",
        "\n",
        "    # steps / cfg / seed\n",
        "    steps = _safe_int(meta.get(\"steps\", None), default=None)\n",
        "    if steps is not None:\n",
        "        w_steps.value = int(max(w_steps.min, min(w_steps.max, steps)))\n",
        "\n",
        "    cfg = _safe_float(meta.get(\"cfg_scale\", None), default=None)\n",
        "    if cfg is not None:\n",
        "        w_cfg.value = float(max(w_cfg.min, min(w_cfg.max, cfg)))\n",
        "\n",
        "    seed = _safe_int(meta.get(\"seed\", None), default=None)\n",
        "    if seed is not None:\n",
        "        w_seed.value = int(seed)\n",
        "\n",
        "    # width / height\n",
        "    width = _safe_int(meta.get(\"width\", None), default=None)\n",
        "    height = _safe_int(meta.get(\"height\", None), default=None)\n",
        "    if width is not None and height is not None:\n",
        "        width16 = _round_to_16(int(width))\n",
        "        height16 = _round_to_16(int(height))\n",
        "        width16 = int(max(w_width.min, min(w_width.max, width16)))\n",
        "        height16 = int(max(w_height.min, min(w_height.max, height16)))\n",
        "\n",
        "        # å°½é‡åŒ¹é…é¢„è®¾ï¼Œå¦åˆ™è‡ªå®šä¹‰\n",
        "        ratio_name = meta.get(\"aspect_ratio_preset\", None)\n",
        "        if not isinstance(ratio_name, str) or ratio_name not in ASPECT_RATIOS:\n",
        "            ratio_name = _set_ratio_preset_if_exact_match(width16, height16)\n",
        "        w_ratio.value = ratio_name\n",
        "\n",
        "        # å¦‚æœæ˜¯è‡ªå®šä¹‰ï¼Œç›´æ¥å†™å…¥ï¼›å¦‚æœæ˜¯é¢„è®¾ï¼Œæ¯”ä¾‹è”åŠ¨ä¼šè‡ªåŠ¨å†™å…¥è¿‘ä¼¼å€¼ï¼Œè¿™é‡Œå†å¼ºåˆ¶å¯¹é½åˆ°å…ƒæ•°æ®å€¼\n",
        "        w_width.value = width16\n",
        "        w_height.value = height16\n",
        "        _update_size_info(None)\n",
        "\n",
        "    # SVH\n",
        "    svh = meta.get(\"seed_variance_enhancer\", {})\n",
        "    if isinstance(svh, dict):\n",
        "        enabled = bool(svh.get(\"enabled\", False))\n",
        "        w_svh_enable.value = enabled\n",
        "\n",
        "        rp = _safe_float(svh.get(\"randomize_percent\", None), default=None)\n",
        "        if rp is not None:\n",
        "            w_svh_randomize_percent.value = float(max(w_svh_randomize_percent.min, min(w_svh_randomize_percent.max, rp)))\n",
        "\n",
        "        st = _safe_float(svh.get(\"strength\", None), default=None)\n",
        "        if st is not None:\n",
        "            w_svh_strength.value = float(st)\n",
        "\n",
        "        ni = svh.get(\"noise_insert\", None)\n",
        "        if isinstance(ni, str) and ni in [o for o in w_svh_noise_insert.options]:\n",
        "            w_svh_noise_insert.value = ni\n",
        "\n",
        "        sw = _safe_float(svh.get(\"steps_switchover_percent\", None), default=None)\n",
        "        if sw is not None:\n",
        "            w_svh_steps_switchover_percent.value = float(max(w_svh_steps_switchover_percent.min, min(w_svh_steps_switchover_percent.max, sw)))\n",
        "\n",
        "        ms = svh.get(\"mask_starts_at\", None)\n",
        "        if isinstance(ms, str) and ms in [o for o in w_svh_mask_starts_at.options]:\n",
        "            w_svh_mask_starts_at.value = ms\n",
        "\n",
        "        mp = _safe_float(svh.get(\"mask_percent\", None), default=None)\n",
        "        if mp is not None:\n",
        "            w_svh_mask_percent.value = float(max(w_svh_mask_percent.min, min(w_svh_mask_percent.max, mp)))\n",
        "\n",
        "        sm = svh.get(\"seed_mode\", None)\n",
        "        if isinstance(sm, str) and sm in [v for _, v in w_svh_seed_mode.options]:\n",
        "            w_svh_seed_mode.value = sm\n",
        "\n",
        "        sseed = _safe_int(svh.get(\"seed\", None), default=None)\n",
        "        if sseed is not None:\n",
        "            w_svh_seed.value = int(sseed)\n",
        "\n",
        "    # LoRA æ§½ä½\n",
        "    lora = meta.get(\"lora_slots\", {})\n",
        "    if isinstance(lora, dict):\n",
        "        # è‹¥å…ƒæ•°æ®å¸¦äº† folderï¼Œåˆ™å›å¡«\n",
        "        lf = lora.get(\"lora_folder\", None)\n",
        "        if isinstance(lf, str) and lf.strip():\n",
        "            w_lora_dir.value = lf.strip()\n",
        "            _refresh_lora_dropdowns()\n",
        "\n",
        "        def _set_if_in_options(dd: widgets.Dropdown, val: str):\n",
        "            vals = [v for _, v in dd.options]\n",
        "            dd.value = val if (val in vals) else \"\"\n",
        "\n",
        "        f1 = lora.get(\"slot1_file\", \"\") or \"\"\n",
        "        f2 = lora.get(\"slot2_file\", \"\") or \"\"\n",
        "        f3 = lora.get(\"slot3_file\", \"\") or \"\"\n",
        "\n",
        "        _set_if_in_options(w_lora1, f1)\n",
        "        _set_if_in_options(w_lora2, f2)\n",
        "        _set_if_in_options(w_lora3, f3)\n",
        "\n",
        "        w1 = _safe_float(lora.get(\"slot1_weight\", None), default=None)\n",
        "        w2 = _safe_float(lora.get(\"slot2_weight\", None), default=None)\n",
        "        w3 = _safe_float(lora.get(\"slot3_weight\", None), default=None)\n",
        "        if w1 is not None: w_lora1_w.value = float(max(w_lora1_w.min, min(w_lora1_w.max, w1)))\n",
        "        if w2 is not None: w_lora2_w.value = float(max(w_lora2_w.min, min(w_lora2_w.max, w2)))\n",
        "        if w3 is not None: w_lora3_w.value = float(max(w_lora3_w.min, min(w_lora3_w.max, w3)))\n",
        "\n",
        "def _copy_text_to_clipboard_js(text: str):\n",
        "    # Colab/Jupyter é€šç”¨ï¼šç”¨ JS å†™å‰ªè´´æ¿ï¼ˆéœ€è¦ç”¨æˆ·æ‰‹åŠ¿è§¦å‘æŒ‰é’®ç‚¹å‡»ï¼Œæ­£å¥½æ»¡è¶³ï¼‰\n",
        "    from IPython.display import Javascript, display as _disp\n",
        "    safe = json.dumps(str(text))\n",
        "    _disp(Javascript(f\"\"\"\n",
        "    (async () => {{\n",
        "      try {{\n",
        "        await navigator.clipboard.writeText({safe});\n",
        "        console.log(\"copied\");\n",
        "      }} catch(e) {{\n",
        "        console.error(e);\n",
        "      }}\n",
        "    }})()\n",
        "    \"\"\"))\n",
        "\n",
        "def _on_load_meta(_):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        png_path = (w_png_meta_path.value or \"\").strip()\n",
        "        if not png_path:\n",
        "            print(\"âŒ è¯·å…ˆå¡«å†™ PNG è·¯å¾„\")\n",
        "            return\n",
        "        if not os.path.exists(png_path):\n",
        "            print(f\"âŒ PNG ä¸å­˜åœ¨: {png_path}\")\n",
        "            return\n",
        "        meta = _read_metadata_from_png(png_path)\n",
        "        if not meta:\n",
        "            print(\"âŒ æœªè¯»åˆ° parameters å…ƒæ•°æ®ï¼ˆè¯¥ PNG å¯èƒ½ä¸æ˜¯æœ¬é¢æ¿ä¿å­˜çš„ï¼Œæˆ–æ²¡æœ‰ parameters å­—æ®µï¼‰\")\n",
        "            return\n",
        "\n",
        "        _apply_metadata_to_ui(meta)\n",
        "\n",
        "        # åŒæ—¶æŠŠå®Œæ•´ JSON æ”¾å…¥å…¨å±€ï¼Œæ–¹ä¾¿å¤åˆ¶/æ£€æŸ¥\n",
        "        globals()[\"_last_loaded_png_metadata\"] = meta\n",
        "\n",
        "        # æ›´æ–°æç¤º\n",
        "        meta_status.value = f\"<div class='zimg-info'>âœ… å·²ä» PNG è¯»å–å¹¶å›å¡«ï¼š{os.path.basename(png_path)}</div>\"\n",
        "        print(\"âœ… å…ƒæ•°æ®å·²å›å¡«åˆ°é¢æ¿å‚æ•°ã€‚\")\n",
        "        print(\"â€” å¯ç›´æ¥ç‚¹ ğŸš€ ç”Ÿæˆå›¾åƒ å¤ç°ã€‚\")\n",
        "        print(\"â€” å¦‚éœ€å¤åˆ¶åŸå§‹ JSONï¼Œç‚¹ã€Œå¤åˆ¶å…ƒæ•°æ® JSON åˆ°å‰ªè´´æ¿ã€ã€‚\")\n",
        "\n",
        "def _on_copy_meta_json(_):\n",
        "    meta = globals().get(\"_last_loaded_png_metadata\", None)\n",
        "    if not isinstance(meta, dict):\n",
        "        meta_status.value = \"<div class='zimg-info'>âš ï¸ å°šæœªè¯»å–ä»»ä½• PNG å…ƒæ•°æ®ï¼›è¯·å…ˆç‚¹å‡»ã€Œä¸€é”®è¯»å–ã€</div>\"\n",
        "        return\n",
        "    text = json.dumps(meta, ensure_ascii=False, indent=2)\n",
        "    _copy_text_to_clipboard_js(text)\n",
        "    meta_status.value = \"<div class='zimg-info'>ğŸ“‹ å·²å°†å…ƒæ•°æ® JSON å¤åˆ¶åˆ°å‰ªè´´æ¿</div>\"\n",
        "\n",
        "btn_load_meta.on_click(_on_load_meta)\n",
        "btn_copy_meta_json.on_click(_on_copy_meta_json)\n",
        "\n",
        "# ==================== 2. å›¾åƒå°ºå¯¸åŒºåŸŸ ====================\n",
        "_current_ratio = None\n",
        "_updating = False\n",
        "\n",
        "w_ratio = widgets.Dropdown(\n",
        "    options=list(ASPECT_RATIOS.keys()),\n",
        "    value=\"è‡ªå®šä¹‰\",\n",
        "    description=\"\",\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "\n",
        "w_width = widgets.IntSlider(\n",
        "    value=1024, min=512, max=2560, step=16,\n",
        "    description=\"å®½åº¦\",\n",
        "    style={'description_width': '60px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    readout=True,\n",
        "    continuous_update=False,\n",
        ")\n",
        "w_height = widgets.IntSlider(\n",
        "    value=1024, min=512, max=2560, step=16,\n",
        "    description=\"é«˜åº¦\",\n",
        "    style={'description_width': '60px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    readout=True,\n",
        "    continuous_update=False,\n",
        ")\n",
        "w_size_info = widgets.HTML('<div class=\"zimg-info\">ğŸ“ æœ€ç»ˆå°ºå¯¸: 1024 Ã— 1024 (å·²å¯¹é½16åƒç´ )</div>')\n",
        "\n",
        "def _parse_ratio_from_name(name):\n",
        "    if name == \"è‡ªå®šä¹‰\":\n",
        "        return None\n",
        "    w, h = ASPECT_RATIOS[name]\n",
        "    return w / h\n",
        "\n",
        "def _set_initial_size_by_ratio(ratio):\n",
        "    if ratio is None:\n",
        "        return 1024, 1024\n",
        "    if ratio >= 1:\n",
        "        w = 1024\n",
        "        h = round(w / ratio)\n",
        "    else:\n",
        "        h = 1024\n",
        "        w = round(h * ratio)\n",
        "    w = max(512, min(2560, _round_to_16(w)))\n",
        "    h = max(512, min(2560, _round_to_16(h)))\n",
        "    return w, h\n",
        "\n",
        "def _on_ratio_change(change):\n",
        "    global _current_ratio, _updating\n",
        "    ratio_name = change[\"new\"]\n",
        "    ratio = _parse_ratio_from_name(ratio_name)\n",
        "    _current_ratio = ratio\n",
        "\n",
        "    if ratio is not None:\n",
        "        w, h = _set_initial_size_by_ratio(ratio)\n",
        "        _updating = True\n",
        "        w_width.value = w\n",
        "        w_height.value = h\n",
        "        _updating = False\n",
        "        _update_size_info(None)\n",
        "    else:\n",
        "        _update_size_info(None)\n",
        "\n",
        "def _on_width_change(change):\n",
        "    global _current_ratio, _updating\n",
        "    if _updating or _current_ratio is None:\n",
        "        _update_size_info(change)\n",
        "        return\n",
        "    new_w = _round_to_16(change[\"new\"])\n",
        "    new_h = _round_to_16(new_w / _current_ratio)\n",
        "    new_h = max(512, min(2560, new_h))\n",
        "    _updating = True\n",
        "    w_height.value = new_h\n",
        "    _updating = False\n",
        "    _update_size_info(None)\n",
        "\n",
        "def _on_height_change(change):\n",
        "    global _current_ratio, _updating\n",
        "    if _updating or _current_ratio is None:\n",
        "        _update_size_info(change)\n",
        "        return\n",
        "    new_h = _round_to_16(change[\"new\"])\n",
        "    new_w = _round_to_16(new_h * _current_ratio)\n",
        "    new_w = max(512, min(2560, new_w))\n",
        "    _updating = True\n",
        "    w_width.value = new_w\n",
        "    _updating = False\n",
        "    _update_size_info(None)\n",
        "\n",
        "def _update_size_info(change):\n",
        "    w16 = _round_to_16(w_width.value)\n",
        "    h16 = _round_to_16(w_height.value)\n",
        "    w_size_info.value = f'<div class=\"zimg-info\">ğŸ“ æœ€ç»ˆå°ºå¯¸: {w16} Ã— {h16} (å·²å¯¹é½16åƒç´ )</div>'\n",
        "\n",
        "w_ratio.observe(_on_ratio_change, names=\"value\")\n",
        "w_width.observe(_on_width_change, names=\"value\")\n",
        "w_height.observe(_on_height_change, names=\"value\")\n",
        "\n",
        "# ==================== 3. ç”Ÿæˆå‚æ•°åŒºåŸŸ ====================\n",
        "w_steps = widgets.IntSlider(\n",
        "    value=9, min=5, max=20, step=1,\n",
        "    description=\"é‡‡æ ·æ­¥æ•°\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "w_cfg = widgets.FloatSlider(\n",
        "    value=0.0, min=0.0, max=2.0, step=0.01,\n",
        "    description=\"CFG å¼ºåº¦\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "w_seed = widgets.IntText(\n",
        "    value=-1,\n",
        "    description=\"éšæœºç§å­\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "\n",
        "# ==================== âœ… 3.5 æ‰¹é‡å¼ æ•°ï¼ˆæ–°å¢ï¼‰ ====================\n",
        "w_batch = widgets.BoundedIntText(\n",
        "    value=1, min=1, max=200, step=1,\n",
        "    description=\"æ¯æ¬¡å¼ æ•°\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"220px\")\n",
        ")\n",
        "\n",
        "# ==================== 4. æ˜¾å­˜ä¼˜åŒ–åŒºåŸŸ ====================\n",
        "w_attn = widgets.Dropdown(\n",
        "    options=[\"auto\", \"max\", 1, 2, 4, 8, 16, 32],\n",
        "    value=\"auto\",\n",
        "    description=\"æ³¨æ„åŠ›åˆ‡ç‰‡\",\n",
        "    style={'description_width': '100px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "w_int8 = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"å¯ç”¨ INT8 é‡åŒ–åŠ é€Ÿ\",\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        ")\n",
        "\n",
        "w_dtype_mode = widgets.Dropdown(\n",
        "    options=[\n",
        "        (\"é»˜è®¤ FP32ï¼ˆåŸºçº¿ï¼‰\", \"fp32\"),\n",
        "        (\"çœæ˜¾å­˜ FP16ï¼ˆæ¨èï¼‰\", \"fp16\"),\n",
        "        (\"çœæ˜¾å­˜ BF16ï¼ˆéœ€GPUæ”¯æŒï¼‰\", \"bf16\"),\n",
        "    ],\n",
        "    value=\"fp32\",\n",
        "    description=\"dtype\",\n",
        "    style={'description_width': '60px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_dtype_apply = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"å¯ç”¨ dtype ä¸‹æ²‰ï¼ˆtransformer/TE ç­‰å°½åŠ›è½¬ä¸º FP16/BF16ï¼‰\",\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "dtype_hint = widgets.HTML(\n",
        "    value=\"<div class='zimg-info'>ğŸ’¡ é»˜è®¤å…³é—­ä»¥ä¸ç ´å 2048Â² åŸºçº¿ã€‚</div>\"\n",
        ")\n",
        "\n",
        "# ==================== 5. VAE è§£ç åŒºåŸŸ ====================\n",
        "w_scaling = widgets.FloatText(\n",
        "    value=0.3611,\n",
        "    description=\"ç¼©æ”¾å› å­\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "w_tiled = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"å¯ç”¨ VAE åˆ†å—è§£ç ï¼ˆå¤§å›¾æ¨èï¼‰\",\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "w_tile = widgets.Dropdown(\n",
        "    options=[\"auto\", 64, 128, 256, 384, 512, 768, 1024],\n",
        "    value=\"auto\",\n",
        "    description=\"åˆ†å—å¤§å°\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "# ==================== 6. ä¿å­˜é€‰é¡¹åŒºåŸŸ ====================\n",
        "w_save_latent = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"ä¿å­˜ Latent (.npz)\",\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width=\"50%\")\n",
        ")\n",
        "w_save_png = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"ä¿å­˜ PNG å›¾åƒ\",\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width=\"50%\")\n",
        ")\n",
        "w_latent_dir = widgets.Text(\n",
        "    value=\"/content/drive/MyDrive/Z-image/Latent\",\n",
        "    description=\"Latent ç›®å½•\",\n",
        "    style={'description_width': '90px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "w_png_dir = widgets.Text(\n",
        "    value=\"/content/drive/MyDrive/Z-image/Outputs\",\n",
        "    description=\"PNG ç›®å½•\",\n",
        "    style={'description_width': '90px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "# ==================== âœ… 6.5. LoRA ä¸‰æ§½ä½ï¼ˆæ¨ç†æ—¶åº”ç”¨ï¼Œä¸æ”¹å…¶å®ƒé€»è¾‘ï¼‰ ====================\n",
        "w_lora_dir = widgets.Text(\n",
        "    value=globals().get(\"LORA_FOLDER\", \"/content/drive/MyDrive/model\"),\n",
        "    description=\"LoRA ç›®å½•\",\n",
        "    style={'description_width': '90px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "def _list_lora_files_for_ui(folder: str):\n",
        "    folder = (folder or \"\").strip()\n",
        "    if not folder or not os.path.exists(folder):\n",
        "        return []\n",
        "    files = []\n",
        "    for fn in os.listdir(folder):\n",
        "        if fn.endswith(\".safetensors\") or fn.endswith(\".pt\") or fn.endswith(\".bin\"):\n",
        "            files.append(fn)\n",
        "    return sorted(files)\n",
        "\n",
        "def _adapter_name_for_slot(i: int) -> str:\n",
        "    return f\"lora_{i}\"\n",
        "\n",
        "def _pipe_has_adapter(p, adapter_name: str) -> bool:\n",
        "    try:\n",
        "        target = getattr(p, \"unet\", None) or getattr(p, \"transformer\", None)\n",
        "        if target is not None and hasattr(target, \"peft_config\") and target.peft_config:\n",
        "            return adapter_name in target.peft_config\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "def _ensure_lora_loaded_if_needed(p, adapter_name: str, folder: str, file_name: str):\n",
        "    if not file_name:\n",
        "        return\n",
        "    if _pipe_has_adapter(p, adapter_name):\n",
        "        return\n",
        "    folder = (folder or \"\").strip()\n",
        "    fp = os.path.join(folder, file_name)\n",
        "    if not os.path.exists(fp):\n",
        "        raise RuntimeError(f\"LoRA æ–‡ä»¶ä¸å­˜åœ¨: {fp}\")\n",
        "    p.load_lora_weights(fp, adapter_name=adapter_name)\n",
        "\n",
        "def _delete_lora_if_present(p, adapter_name: str):\n",
        "    try:\n",
        "        p.delete_adapters([adapter_name])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _apply_lora_slots_to_pipe(p, slot_files: list, slot_weights: list, lora_folder: str):\n",
        "    for i in range(1, 4):\n",
        "        fn = slot_files[i-1]\n",
        "        adapter = _adapter_name_for_slot(i)\n",
        "        if not fn:\n",
        "            _delete_lora_if_present(p, adapter)\n",
        "\n",
        "    for i in range(1, 4):\n",
        "        fn = slot_files[i-1]\n",
        "        adapter = _adapter_name_for_slot(i)\n",
        "        if fn:\n",
        "            _ensure_lora_loaded_if_needed(p, adapter, folder=lora_folder, file_name=fn)\n",
        "\n",
        "    active_adapters = []\n",
        "    active_weights = []\n",
        "    for i in range(1, 4):\n",
        "        fn = slot_files[i-1]\n",
        "        w = float(slot_weights[i-1])\n",
        "        if fn and w != 0.0:\n",
        "            active_adapters.append(_adapter_name_for_slot(i))\n",
        "            active_weights.append(w)\n",
        "\n",
        "    if not active_adapters:\n",
        "        try:\n",
        "            if hasattr(p, \"set_adapters\"):\n",
        "                p.set_adapters([], [])\n",
        "        except Exception:\n",
        "            pass\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        if hasattr(p, \"set_adapters\"):\n",
        "            p.set_adapters(active_adapters, active_weights)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _make_lora_slot_ui(slot_idx: int):\n",
        "    dd = widgets.Dropdown(\n",
        "        options=[(\"ï¼ˆä¸åŠ è½½ï¼‰\", \"\")],\n",
        "        value=\"\",\n",
        "        description=f\"æ§½ä½{slot_idx}\",\n",
        "        layout=widgets.Layout(width=\"100%\")\n",
        "    )\n",
        "    wt = widgets.FloatSlider(\n",
        "        value=0.0, min=-2.0, max=2.0, step=0.05,\n",
        "        description=\"å¼ºåº¦\",\n",
        "        style={'description_width': '60px'},\n",
        "        layout=widgets.Layout(width=\"100%\"),\n",
        "        readout=True,\n",
        "        continuous_update=False,\n",
        "    )\n",
        "    return dd, wt\n",
        "\n",
        "w_lora1, w_lora1_w = _make_lora_slot_ui(1)\n",
        "w_lora2, w_lora2_w = _make_lora_slot_ui(2)\n",
        "w_lora3, w_lora3_w = _make_lora_slot_ui(3)\n",
        "\n",
        "btn_lora_refresh = widgets.Button(\n",
        "    description=\"ğŸ”„ åˆ·æ–° LoRA æ–‡ä»¶åˆ—è¡¨\",\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(width=\"220px\", height=\"32px\")\n",
        ")\n",
        "\n",
        "def _refresh_lora_dropdowns(_=None):\n",
        "    folder = w_lora_dir.value.strip()\n",
        "    files = _list_lora_files_for_ui(folder)\n",
        "    opts = [(\"ï¼ˆä¸åŠ è½½ï¼‰\", \"\")] + [(fn, fn) for fn in files]\n",
        "\n",
        "    cur1, cur2, cur3 = w_lora1.value, w_lora2.value, w_lora3.value\n",
        "    w_lora1.options = opts\n",
        "    w_lora2.options = opts\n",
        "    w_lora3.options = opts\n",
        "\n",
        "    values = [v for _, v in opts]\n",
        "    w_lora1.value = cur1 if cur1 in values else \"\"\n",
        "    w_lora2.value = cur2 if cur2 in values else \"\"\n",
        "    w_lora3.value = cur3 if cur3 in values else \"\"\n",
        "\n",
        "    try:\n",
        "        ll = globals().get(\"loaded_loras\", {})\n",
        "        if isinstance(ll, dict):\n",
        "            if ll.get(\"lora_1\") in values: w_lora1.value = ll[\"lora_1\"]\n",
        "            if ll.get(\"lora_2\") in values: w_lora2.value = ll[\"lora_2\"]\n",
        "            if ll.get(\"lora_3\") in values: w_lora3.value = ll[\"lora_3\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "btn_lora_refresh.on_click(_refresh_lora_dropdowns)\n",
        "\n",
        "def _on_lora_dir_change(change):\n",
        "    _refresh_lora_dropdowns()\n",
        "\n",
        "w_lora_dir.observe(_on_lora_dir_change, names=\"value\")\n",
        "_refresh_lora_dropdowns()\n",
        "\n",
        "lora_slots_ui = widgets.VBox([\n",
        "    widgets.HTML('<div class=\"zimg-label\">LoRA æ§½ä½ï¼ˆæ¨ç†å‰è‡ªåŠ¨åŠ è½½/å¸è½½ï¼›æ§½ä½æ˜¾ç¤ºæ–‡ä»¶åï¼Œé¿å…è¯¯ç”¨ï¼‰</div>'),\n",
        "    w_lora_dir,\n",
        "    widgets.HTML(\"<div class='zimg-info'>âš ï¸ æ³¨æ„ï¼šåŠ è½½ LoRA å¯èƒ½å¯¼è‡´ 2048Â² æ˜¾å­˜ä¸å¤Ÿï¼›å¯é™åˆ†è¾¨ç‡ç»§ç»­ç”¨ã€‚</div>\"),\n",
        "    btn_lora_refresh,\n",
        "    widgets.VBox([w_lora1, w_lora1_w, w_lora2, w_lora2_w, w_lora3, w_lora3_w])\n",
        "])\n",
        "\n",
        "# ==================== âœ… 6.6 SeedVarianceEnhancer æŠ˜å æ¨¡å— ====================\n",
        "w_svh_enable = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"å¯ç”¨ SeedVarianceEnhancerï¼ˆå¯¹ prompt embeds åŠ å™ªå¢å¼ºå¤šæ ·æ€§ï¼‰\",\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_randomize_percent = widgets.FloatSlider(\n",
        "    value=50.0, min=1.0, max=100.0, step=1.0,\n",
        "    description=\"randomize%\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    readout=True,\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "w_svh_strength = widgets.FloatText(\n",
        "    value=20.0,\n",
        "    description=\"strength\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_noise_insert = widgets.Dropdown(\n",
        "    options=[\"noise on beginning steps\", \"noise on ending steps\", \"noise on all steps\", \"disabled\"],\n",
        "    value=\"noise on all steps\",\n",
        "    description=\"noise_insert\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_steps_switchover_percent = widgets.FloatSlider(\n",
        "    value=20.0, min=1.0, max=99.0, step=1.0,\n",
        "    description=\"switch%\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    readout=True,\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "w_svh_seed_mode = widgets.Dropdown(\n",
        "    options=[(\"è·Ÿéšå‡ºå›¾ç§å­\", \"follow\"), (\"è‡ªå®šä¹‰ SVH ç§å­\", \"custom\")],\n",
        "    value=\"follow\",\n",
        "    description=\"SVH seed\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_seed = widgets.IntText(\n",
        "    value=0,\n",
        "    description=\"seed\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_mask_starts_at = widgets.Dropdown(\n",
        "    options=[\"beginning\", \"end\"],\n",
        "    value=\"beginning\",\n",
        "    description=\"mask_from\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "w_svh_mask_percent = widgets.FloatSlider(\n",
        "    value=0.0, min=0.0, max=99.0, step=1.0,\n",
        "    description=\"mask%\",\n",
        "    style={'description_width': '110px'},\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    readout=True,\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "w_svh_log_to_console = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"log_to_consoleï¼ˆæ‰“å° embedding ç»Ÿè®¡ç­‰ï¼‰\",\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "def _svh_controls_enabled(enable: bool):\n",
        "    for w in [\n",
        "        w_svh_randomize_percent, w_svh_strength, w_svh_noise_insert,\n",
        "        w_svh_steps_switchover_percent, w_svh_seed_mode, w_svh_seed,\n",
        "        w_svh_mask_starts_at, w_svh_mask_percent, w_svh_log_to_console\n",
        "    ]:\n",
        "        w.disabled = not bool(enable)\n",
        "\n",
        "def _on_svh_enable_change(change):\n",
        "    _svh_controls_enabled(change[\"new\"])\n",
        "    _on_svh_seed_mode_change({\"new\": w_svh_seed_mode.value})\n",
        "\n",
        "def _on_svh_seed_mode_change(change):\n",
        "    w_svh_seed.disabled = (change[\"new\"] != \"custom\") or (not bool(w_svh_enable.value))\n",
        "\n",
        "w_svh_enable.observe(_on_svh_enable_change, names=\"value\")\n",
        "w_svh_seed_mode.observe(_on_svh_seed_mode_change, names=\"value\")\n",
        "_svh_controls_enabled(w_svh_enable.value)\n",
        "_on_svh_seed_mode_change({\"new\": w_svh_seed_mode.value})\n",
        "\n",
        "def _svh_apply_to_embeds(\n",
        "    embeds_kwargs: dict,\n",
        "    *,\n",
        "    enable: bool,\n",
        "    randomize_percent: float,\n",
        "    strength: float,\n",
        "    noise_insert: str,\n",
        "    steps_switchover_percent: float,\n",
        "    seed: int,\n",
        "    mask_starts_at: str,\n",
        "    mask_percent: float,\n",
        "    log_to_console: bool,\n",
        "):\n",
        "    if not enable:\n",
        "        return embeds_kwargs\n",
        "    if noise_insert == \"disabled\":\n",
        "        return embeds_kwargs\n",
        "    if strength == 0:\n",
        "        return embeds_kwargs\n",
        "    if \"prompt_embeds\" not in embeds_kwargs or not isinstance(embeds_kwargs[\"prompt_embeds\"], torch.Tensor):\n",
        "        return embeds_kwargs\n",
        "\n",
        "    rp = max(1.0, min(100.0, float(randomize_percent))) / 100.0\n",
        "    mp = max(0.0, min(99.0, float(mask_percent))) / 100.0\n",
        "\n",
        "    if noise_insert != \"noise on all steps\":\n",
        "        if log_to_console:\n",
        "            print(f\"âš ï¸ SVH: å½“å‰æ¨ç†ç®¡çº¿ä¸æ”¯æŒæŒ‰æ­¥æ®µåˆ‡æ¢ï¼ˆ{noise_insert}ï¼‰ï¼Œå·²æŒ‰ 'noise on all steps' å¤„ç†ã€‚\")\n",
        "\n",
        "    def _first_null_last_nonnull(seq_t: torch.Tensor):\n",
        "        first_null = -1\n",
        "        last_nonnull = -1\n",
        "        null_seq = [0] * seq_t.size(1)\n",
        "        for i in range(seq_t.size(1)):\n",
        "            s = seq_t[:, i, :]\n",
        "            is_all_zero = bool(torch.all(s == 0).item())\n",
        "            null_seq[i] = 0 if is_all_zero else 1\n",
        "            if not is_all_zero:\n",
        "                last_nonnull = i\n",
        "            if is_all_zero and first_null == -1:\n",
        "                first_null = i\n",
        "        return first_null, last_nonnull, null_seq\n",
        "\n",
        "    def _apply_one(t: torch.Tensor, local_seed: int):\n",
        "        device = t.device\n",
        "        torch.manual_seed(int(local_seed))\n",
        "        noise = torch.rand_like(t) * 2 * float(strength) - float(strength)\n",
        "\n",
        "        torch.manual_seed(int(local_seed) + 1)\n",
        "        noise_mask = torch.bernoulli(torch.ones_like(t) * rp).bool()\n",
        "\n",
        "        first_null, last_nonnull, null_seq = _first_null_last_nonnull(t)\n",
        "\n",
        "        if mp > 0 or (last_nonnull < t.size(1) - 1 and last_nonnull >= 0):\n",
        "            if last_nonnull < t.size(1) - 1 and last_nonnull >= 0:\n",
        "                seq_len = last_nonnull + 1\n",
        "            else:\n",
        "                seq_len = t.size(1)\n",
        "\n",
        "            if mask_starts_at == \"end\":\n",
        "                mask_start = seq_len - int(seq_len * mp)\n",
        "                mask_end = t.size(1)\n",
        "            else:\n",
        "                mask_start = 0\n",
        "                mask_end = int(seq_len * mp)\n",
        "\n",
        "            prompt_mask = torch.arange(t.size(1), device=device).view(1, -1, 1).expand(t.size(0), -1, t.size(2))\n",
        "            prompt_mask = (prompt_mask >= mask_start) & (prompt_mask < mask_end)\n",
        "\n",
        "            if first_null > -1:\n",
        "                null_mask_tensor = ~torch.tensor(null_seq, device=device, dtype=torch.bool)\n",
        "                null_mask_tensor = null_mask_tensor.view(1, -1, 1).expand(t.size(0), -1, t.size(2))\n",
        "                prompt_mask = prompt_mask | null_mask_tensor\n",
        "\n",
        "            noise_mask = noise_mask & (~prompt_mask)\n",
        "\n",
        "        out = t + (noise * noise_mask)\n",
        "\n",
        "        if log_to_console:\n",
        "            with torch.no_grad():\n",
        "                st = float(torch.std(t).item())\n",
        "                print(f\"SVH: applied noise | seed={local_seed} | rp={rp:.2f} | strength={strength} | std={st:.6f} | suggest {st/10:.6f}-{st*10:.6f}\")\n",
        "\n",
        "        return out\n",
        "\n",
        "    embeds_kwargs = dict(embeds_kwargs)\n",
        "    embeds_kwargs[\"prompt_embeds\"] = _apply_one(embeds_kwargs[\"prompt_embeds\"], seed)\n",
        "\n",
        "    if \"negative_prompt_embeds\" in embeds_kwargs and isinstance(embeds_kwargs[\"negative_prompt_embeds\"], torch.Tensor):\n",
        "        embeds_kwargs[\"negative_prompt_embeds\"] = _apply_one(embeds_kwargs[\"negative_prompt_embeds\"], seed + 999)\n",
        "\n",
        "    return embeds_kwargs\n",
        "\n",
        "# ==================== 7. æ“ä½œæŒ‰é’® ====================\n",
        "btn_run = widgets.Button(\n",
        "    description=\"ğŸš€ ç”Ÿæˆå›¾åƒ\",\n",
        "    button_style=\"success\",\n",
        "    layout=widgets.Layout(width=\"160px\", height=\"48px\"),\n",
        ")\n",
        "btn_clear = widgets.Button(\n",
        "    description=\"ğŸ—‘ï¸ æ¸…ç©ºè¾“å‡º\",\n",
        "    button_style=\"warning\",\n",
        "    layout=widgets.Layout(width=\"140px\", height=\"48px\"),\n",
        ")\n",
        "\n",
        "out = widgets.Output(layout=widgets.Layout(\n",
        "    border=\"2px solid #334155\",\n",
        "    border_radius=\"12px\",\n",
        "    padding=\"16px\",\n",
        "    min_height=\"300px\",\n",
        "    background=\"#0f172a\",\n",
        "))\n",
        "\n",
        "# ======================================================\n",
        "# âœ… å…ƒæ•°æ®æ„å»ºä¸å†™å…¥ï¼ˆå·²ä¿®å¤ï¼‰\n",
        "# ======================================================\n",
        "def _build_generation_metadata(\n",
        "    prompt: str,\n",
        "    negative: str,\n",
        "    seed: int,\n",
        "    steps: int,\n",
        "    cfg: float,\n",
        "    width: int,\n",
        "    height: int,\n",
        "    ratio_name: str,\n",
        "    svh_meta: dict | None,\n",
        "    lora_slots: dict | None,\n",
        "    pipe=None,\n",
        ") -> dict:\n",
        "    \"\"\"æ„å»ºä»…åŒ…å«å‡ºå›¾å‚æ•°å’Œ SVH å‚æ•°çš„å…ƒæ•°æ®å­—å…¸\"\"\"\n",
        "    meta = {\n",
        "        \"prompt\": prompt,\n",
        "        \"negative_prompt\": negative,\n",
        "        \"seed\": int(seed),\n",
        "        \"steps\": int(steps),\n",
        "        \"cfg_scale\": float(cfg),\n",
        "        \"width\": int(width),\n",
        "        \"height\": int(height),\n",
        "        \"aspect_ratio_preset\": ratio_name,\n",
        "        \"lora_slots\": lora_slots or {},\n",
        "        \"seed_variance_enhancer\": svh_meta or {},\n",
        "        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "\n",
        "    if pipe is not None:\n",
        "        try:\n",
        "            meta[\"pipeline_class\"] = pipe.__class__.__name__\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if hasattr(pipe, \"scheduler\") and pipe.scheduler is not None:\n",
        "                meta[\"scheduler_class\"] = pipe.scheduler.__class__.__name__\n",
        "        except Exception:\n",
        "            pass\n",
        "        loras = []\n",
        "        try:\n",
        "            target = getattr(pipe, \"unet\", None) or getattr(pipe, \"transformer\", None)\n",
        "            if target is not None and hasattr(target, \"peft_config\") and target.peft_config:\n",
        "                loras = list(target.peft_config.keys())\n",
        "        except Exception:\n",
        "            pass\n",
        "        meta[\"active_lora_adapters\"] = loras\n",
        "\n",
        "    return meta\n",
        "\n",
        "def _save_latent_with_metadata(\n",
        "    latent: torch.Tensor,\n",
        "    metadata: dict,\n",
        "    save_path: str,\n",
        ") -> bool:\n",
        "    try:\n",
        "        lat_np = latent.detach().cpu().to(torch.float32).numpy()\n",
        "        metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2)\n",
        "        np.savez_compressed(\n",
        "            save_path,\n",
        "            latent=lat_np,\n",
        "            metadata_json=np.array([metadata_json], dtype=object),\n",
        "        )\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¿å­˜ Latent å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def _save_png_with_metadata(\n",
        "    image: Image.Image,\n",
        "    metadata: dict,\n",
        "    save_path: str,\n",
        ") -> bool:\n",
        "    try:\n",
        "        from PIL import PngImagePlugin\n",
        "        pnginfo = PngImagePlugin.PngInfo()\n",
        "\n",
        "        metadata_json = json.dumps(metadata, ensure_ascii=False, indent=2)\n",
        "        pnginfo.add_text(\"parameters\", metadata_json)\n",
        "\n",
        "        pnginfo.add_text(\"prompt\", str(metadata.get(\"prompt\", \"\")))\n",
        "        pnginfo.add_text(\"negative_prompt\", str(metadata.get(\"negative_prompt\", \"\")))\n",
        "        pnginfo.add_text(\"seed\", str(metadata.get(\"seed\", \"\")))\n",
        "        pnginfo.add_text(\"steps\", str(metadata.get(\"steps\", \"\")))\n",
        "        pnginfo.add_text(\"cfg_scale\", str(metadata.get(\"cfg_scale\", \"\")))\n",
        "        pnginfo.add_text(\"width\", str(metadata.get(\"width\", \"\")))\n",
        "        pnginfo.add_text(\"height\", str(metadata.get(\"height\", \"\")))\n",
        "\n",
        "        svh = metadata.get(\"seed_variance_enhancer\", {})\n",
        "        if svh.get(\"enabled\"):\n",
        "            pnginfo.add_text(\"svh_enabled\", \"true\")\n",
        "            pnginfo.add_text(\"svh_strength\", str(svh.get(\"strength\", \"\")))\n",
        "            pnginfo.add_text(\"svh_randomize_percent\", str(svh.get(\"randomize_percent\", \"\")))\n",
        "            pnginfo.add_text(\"svh_seed\", str(svh.get(\"seed\", \"\")))\n",
        "\n",
        "        lora_slots = metadata.get(\"lora_slots\", {})\n",
        "        for slot_key in [\"slot1_file\", \"slot2_file\", \"slot3_file\"]:\n",
        "            if lora_slots.get(slot_key):\n",
        "                pnginfo.add_text(f\"lora_{slot_key}\", str(lora_slots[slot_key]))\n",
        "\n",
        "        image.save(save_path, pnginfo=pnginfo)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¿å­˜ PNG å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def _read_metadata_from_latent(npz_path: str) -> dict | None:\n",
        "    try:\n",
        "        data = np.load(npz_path, allow_pickle=True)\n",
        "        if \"metadata_json\" in data:\n",
        "            json_str = data[\"metadata_json\"]\n",
        "            if isinstance(json_str, np.ndarray):\n",
        "                json_str = json_str.item() if json_str.ndim == 0 else json_str[0]\n",
        "            return json.loads(json_str)\n",
        "    except Exception as e:\n",
        "        print(f\"è¯»å– latent å…ƒæ•°æ®å¤±è´¥: {e}\")\n",
        "    return None\n",
        "\n",
        "# ========= ç”Ÿæˆé€»è¾‘ =========\n",
        "def _on_run(_):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        sdpa_ctx = None\n",
        "        try:\n",
        "            p = _get_existing_pipe()\n",
        "            if p is None:\n",
        "                print(\"âŒ æœªæ£€æµ‹åˆ° pipeï¼šè¯·å…ˆç”¨æ¨¡å‹ç®¡ç†å™¨åŠ è½½æ¨¡å‹\")\n",
        "                return\n",
        "\n",
        "            prompt = prompt_box.value.strip()\n",
        "            negative = neg_box.value.strip()\n",
        "            width = int(w_width.value)\n",
        "            height = int(w_height.value)\n",
        "            steps = int(w_steps.value)\n",
        "            cfg = float(w_cfg.value)\n",
        "            seed_in = int(w_seed.value)\n",
        "            batch_n = int(w_batch.value)\n",
        "\n",
        "            attn_slice = w_attn.value\n",
        "            int8_enable = bool(w_int8.value)\n",
        "            scaling = float(w_scaling.value)\n",
        "            tiled = bool(w_tiled.value)\n",
        "            tile_size = w_tile.value\n",
        "\n",
        "            save_latent = bool(w_save_latent.value)\n",
        "            save_png = bool(w_save_png.value)\n",
        "            latent_dir = w_latent_dir.value.strip()\n",
        "            png_dir = w_png_dir.value.strip()\n",
        "\n",
        "            os.makedirs(latent_dir, exist_ok=True)\n",
        "            os.makedirs(png_dir, exist_ok=True)\n",
        "\n",
        "            width16 = _round_to_16(width)\n",
        "            height16 = _round_to_16(height)\n",
        "\n",
        "            if tile_size == \"auto\":\n",
        "                tile_size = _auto_tile_for_size(width16, height16)\n",
        "\n",
        "            dtype_mode = str(w_dtype_mode.value)\n",
        "            target_dtype = _dtype_from_mode(dtype_mode)\n",
        "            if target_dtype == torch.bfloat16 and not _bf16_is_supported_cuda():\n",
        "                print(\"âš ï¸ BF16 ä¼¼ä¹ä¸æ”¯æŒï¼Œè‡ªåŠ¨é™çº§ä¸º FP16\")\n",
        "                target_dtype = torch.float16\n",
        "            dtype_apply_enabled = bool(w_dtype_apply.value)\n",
        "\n",
        "            svh_enabled = bool(w_svh_enable.value)\n",
        "            svh_seed_mode = str(w_svh_seed_mode.value)\n",
        "\n",
        "            print(\"=\" * 50)\n",
        "            print(f\"ğŸ¯ æ‰¹é‡ç”Ÿæˆ: {batch_n} å¼ \")\n",
        "            print(f\"   å°ºå¯¸: {width16}Ã—{height16} | æ­¥æ•°: {steps} | CFG: {cfg}\")\n",
        "            print(f\"   allocator: {_get_allocator_conf() or '(not set)'}\")\n",
        "            print(f\"   dtype_mode={dtype_mode} | apply={dtype_apply_enabled}\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                try: torch.cuda.reset_peak_memory_stats()\n",
        "                except Exception: pass\n",
        "                _print_cuda_mem_line(\"before\")\n",
        "\n",
        "            _prepare_full_gpu(p, attn_slice)\n",
        "            print(f\"âœ… GPU å‡†å¤‡å®Œæˆï¼Œæ³¨æ„åŠ›åˆ‡ç‰‡: {('max' if attn_slice=='auto' else attn_slice)}\")\n",
        "\n",
        "            _maybe_cast_pipe_modules(p, target_dtype=target_dtype, enable=dtype_apply_enabled)\n",
        "\n",
        "            sdpa_ctx = _enter_sdpa_efficient()\n",
        "            if sdpa_ctx is not None:\n",
        "                print(\"âœ… SDPA: EFFICIENT_ATTENTION ä¼˜å…ˆ å·²å¯ç”¨\")\n",
        "            else:\n",
        "                print(\"âš ï¸ SDPA: æœªèƒ½æ˜¾å¼å¯ç”¨ï¼ˆå°†æŒ‰é»˜è®¤å®ç°/åˆ‡ç‰‡è¿è¡Œï¼‰\")\n",
        "\n",
        "            ok_int8, msg_int8 = _apply_int8_matmul_to_transformer(p, int8_enable)\n",
        "            print((\"âœ… \" if ok_int8 else \"âš ï¸ \") + msg_int8)\n",
        "\n",
        "            # LoRAï¼šæ‰¹é‡å¼€å§‹å‰åº”ç”¨ä¸€æ¬¡\n",
        "            slot_files = [w_lora1.value or None, w_lora2.value or None, w_lora3.value or None]\n",
        "            slot_weights = [float(w_lora1_w.value), float(w_lora2_w.value), float(w_lora3_w.value)]\n",
        "\n",
        "            try:\n",
        "                _apply_lora_slots_to_pipe(\n",
        "                    p,\n",
        "                    slot_files=slot_files,\n",
        "                    slot_weights=slot_weights,\n",
        "                    lora_folder=w_lora_dir.value.strip(),\n",
        "                )\n",
        "                active = [(i+1, slot_files[i], slot_weights[i]) for i in range(3) if slot_files[i]]\n",
        "                if active:\n",
        "                    print(\"ğŸ­ LoRA å·²åº”ç”¨æ§½ä½:\")\n",
        "                    for s, fn, wv in active:\n",
        "                        print(f\"   - æ§½ä½{s}: {fn} | strength={wv}\")\n",
        "                else:\n",
        "                    print(\"ğŸ­ LoRA: æœªåŠ è½½ï¼ˆä¿æŒåŸºçº¿æ˜¾å­˜ï¼‰\")\n",
        "            except Exception as e:\n",
        "                print(\"âš ï¸ LoRA åº”ç”¨å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼Œä¸å½±å“æ— LoRAæ¨ç†ï¼‰:\", e)\n",
        "\n",
        "            lora_slots_meta = {\n",
        "                \"slot1_file\": slot_files[0] or \"\",\n",
        "                \"slot1_weight\": slot_weights[0],\n",
        "                \"slot2_file\": slot_files[1] or \"\",\n",
        "                \"slot2_weight\": slot_weights[1],\n",
        "                \"slot3_file\": slot_files[2] or \"\",\n",
        "                \"slot3_weight\": slot_weights[2],\n",
        "                \"lora_folder\": w_lora_dir.value.strip(),\n",
        "            }\n",
        "\n",
        "            for i in range(batch_n):\n",
        "                latent = None\n",
        "                try:\n",
        "                    real_seed = int(np.random.randint(0, 2**32 - 1)) if seed_in == -1 else int(seed_in + i)\n",
        "                    svh_seed = int(w_svh_seed.value) if (svh_seed_mode == \"custom\") else int(real_seed)\n",
        "\n",
        "                    svh_meta = {\n",
        "                        \"enabled\": svh_enabled,\n",
        "                        \"randomize_percent\": float(w_svh_randomize_percent.value),\n",
        "                        \"strength\": float(w_svh_strength.value),\n",
        "                        \"noise_insert\": str(w_svh_noise_insert.value),\n",
        "                        \"steps_switchover_percent\": float(w_svh_steps_switchover_percent.value),\n",
        "                        \"seed_mode\": str(w_svh_seed_mode.value),\n",
        "                        \"seed\": int(svh_seed),\n",
        "                        \"mask_starts_at\": str(w_svh_mask_starts_at.value),\n",
        "                        \"mask_percent\": float(w_svh_mask_percent.value),\n",
        "                    }\n",
        "\n",
        "                    print(\"-\" * 50)\n",
        "                    print(f\"ğŸ–¼ï¸ [{i+1}/{batch_n}] seed={real_seed}\")\n",
        "\n",
        "                    t0 = time.time()\n",
        "                    with torch.inference_mode():\n",
        "                        te_dtype = target_dtype if dtype_apply_enabled else torch.float16\n",
        "\n",
        "                        embeds_kwargs = _encode_prompt_embeds_then_drop_te(\n",
        "                            p, prompt_text=prompt, negative_text=negative, cfg=cfg, te_dtype=te_dtype\n",
        "                        )\n",
        "\n",
        "                        embeds_kwargs = _svh_apply_to_embeds(\n",
        "                            embeds_kwargs,\n",
        "                            enable=svh_enabled,\n",
        "                            randomize_percent=float(w_svh_randomize_percent.value),\n",
        "                            strength=float(w_svh_strength.value),\n",
        "                            noise_insert=str(w_svh_noise_insert.value),\n",
        "                            steps_switchover_percent=float(w_svh_steps_switchover_percent.value),\n",
        "                            seed=int(svh_seed),\n",
        "                            mask_starts_at=str(w_svh_mask_starts_at.value),\n",
        "                            mask_percent=float(w_svh_mask_percent.value),\n",
        "                            log_to_console=bool(w_svh_log_to_console.value),\n",
        "                        )\n",
        "\n",
        "                        if torch.cuda.is_available():\n",
        "                            runtime_fragmentation_cleanup(synchronize=True, reset_peak=True)\n",
        "\n",
        "                        gen = torch.Generator(device=\"cpu\").manual_seed(real_seed)\n",
        "                        result = p(\n",
        "                            prompt=None,\n",
        "                            **embeds_kwargs,\n",
        "                            height=height16,\n",
        "                            width=width16,\n",
        "                            num_inference_steps=steps,\n",
        "                            guidance_scale=cfg,\n",
        "                            generator=gen,\n",
        "                            output_type=\"latent\",\n",
        "                        )\n",
        "                        latent = result.images[0]\n",
        "\n",
        "                    img = _decode_latent_with_pipe_vae(p, latent, scaling_factor=scaling, tiled=tiled, tile_size=tile_size)\n",
        "                    display(img)\n",
        "\n",
        "                    metadata = _build_generation_metadata(\n",
        "                        prompt=prompt,\n",
        "                        negative=negative,\n",
        "                        seed=real_seed,\n",
        "                        steps=steps,\n",
        "                        cfg=cfg,\n",
        "                        width=width16,\n",
        "                        height=height16,\n",
        "                        ratio_name=w_ratio.value,\n",
        "                        svh_meta=svh_meta,\n",
        "                        lora_slots=lora_slots_meta,\n",
        "                        pipe=p,\n",
        "                    )\n",
        "\n",
        "                    ts = datetime.now().strftime(\"%H%M%S\")\n",
        "                    base_name = f\"{real_seed}_{width16}x{height16}_{steps}s_{ts}\"\n",
        "\n",
        "                    if save_latent:\n",
        "                        latent_path = os.path.join(latent_dir, f\"latent_{base_name}.npz\")\n",
        "                        if _save_latent_with_metadata(latent, metadata, latent_path):\n",
        "                            print(f\"ğŸ’¾ Latent: {latent_path}\")\n",
        "                            verify_meta = _read_metadata_from_latent(latent_path)\n",
        "                            if verify_meta:\n",
        "                                print(f\"   âœ“ å…ƒæ•°æ®éªŒè¯æˆåŠŸ (seed={verify_meta.get('seed')})\")\n",
        "\n",
        "                    if save_png:\n",
        "                        png_path = os.path.join(png_dir, f\"img_{base_name}.png\")\n",
        "                        if _save_png_with_metadata(img, metadata, png_path):\n",
        "                            print(f\"ğŸ’¾ PNG: {png_path}\")\n",
        "                            verify_meta = _read_metadata_from_png(png_path)\n",
        "                            if verify_meta:\n",
        "                                print(f\"   âœ“ å…ƒæ•°æ®éªŒè¯æˆåŠŸ (seed={verify_meta.get('seed')})\")\n",
        "\n",
        "                    dt = time.time() - t0\n",
        "                    print(f\"â±ï¸ å•å¼ è€—æ—¶: {dt:.2f} ç§’\")\n",
        "\n",
        "                except Exception:\n",
        "                    traceback.print_exc()\n",
        "                finally:\n",
        "                    try:\n",
        "                        del latent\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    if torch.cuda.is_available():\n",
        "                        try: torch.cuda.empty_cache()\n",
        "                        except Exception: pass\n",
        "                    gc.collect()\n",
        "\n",
        "            print(\"=\" * 50)\n",
        "            print(\"âœ¨ æ‰¹é‡å®Œæˆï¼\")\n",
        "\n",
        "        except Exception:\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            _exit_sdpa(sdpa_ctx)\n",
        "            if torch.cuda.is_available():\n",
        "                try: torch.cuda.empty_cache()\n",
        "                except Exception: pass\n",
        "            gc.collect()\n",
        "\n",
        "def _on_clear(_):\n",
        "    out.clear_output()\n",
        "\n",
        "# ==================== UI å¸ƒå±€å·¥å…· ====================\n",
        "def make_section(title_html, *children):\n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(f'<div class=\"zimg-section-title\">{title_html}</div>'),\n",
        "        *children,\n",
        "    ], layout=widgets.Layout(\n",
        "        padding=\"16px\",\n",
        "        margin=\"8px 0\",\n",
        "        border=\"1px solid #334155\",\n",
        "        border_radius=\"12px\",\n",
        "        background=\"#1e293b\",\n",
        "    ))\n",
        "\n",
        "def make_collapsible_section(title_html, content_widget: widgets.Widget, open_default: bool = True):\n",
        "    acc = widgets.Accordion(children=[content_widget])\n",
        "    acc.set_title(0, title_html)\n",
        "    acc.selected_index = 0 if open_default else None\n",
        "    acc.layout = widgets.Layout(margin=\"8px 0\")\n",
        "    try:\n",
        "        acc.style = {\"_dom_classes\": [\"zimg-accordion\"]}\n",
        "    except Exception:\n",
        "        pass\n",
        "    return acc\n",
        "\n",
        "# ==================== Sections ====================\n",
        "prompt_section = make_section(\n",
        "    \"ğŸ“ æç¤ºè¯\",\n",
        "    widgets.HTML('<div class=\"zimg-label\">æ­£å‘æç¤ºè¯</div>'),\n",
        "    prompt_box,\n",
        "    widgets.HTML('<div class=\"zimg-label\" style=\"margin-top:12px;\">è´Ÿé¢æç¤ºè¯ï¼ˆCFGâ‰¤1æ— æ•ˆï¼‰</div>'),\n",
        "    neg_box,\n",
        "\n",
        "    # âœ… æ–°å¢ï¼šä½ç½®æ”¾åœ¨è´Ÿå‘æç¤ºè¯ä¸‹é¢\n",
        "    widgets.HTML('<div class=\"zimg-label\" style=\"margin-top:12px;\">ğŸ“¥ ä¸€é”®å¤åˆ¶/å›å¡« PNG å…ƒæ•°æ®</div>'),\n",
        "    w_png_meta_path,\n",
        "    widgets.HBox([btn_load_meta, btn_copy_meta_json], layout=widgets.Layout(gap=\"12px\")),\n",
        "    meta_status,\n",
        ")\n",
        "\n",
        "size_section = make_section(\n",
        "    \"ğŸ“ å›¾åƒå°ºå¯¸\",\n",
        "    widgets.HTML('<div class=\"zimg-label\">é¢„è®¾æ¯”ä¾‹ï¼ˆé€‰æ‹©åä¿æŒæ¯”ä¾‹è”åŠ¨ï¼Œæˆ–é€‰\\\"è‡ªå®šä¹‰\\\"è‡ªç”±è°ƒæ•´ï¼‰</div>'),\n",
        "    w_ratio,\n",
        "    w_width,\n",
        "    w_height,\n",
        "    w_size_info,\n",
        ")\n",
        "\n",
        "gen_section = make_section(\n",
        "    \"âš™ï¸ ç”Ÿæˆå‚æ•°\",\n",
        "    w_steps,\n",
        "    w_cfg,\n",
        "    w_seed,\n",
        "    widgets.HTML('<div class=\"zimg-info\">ğŸ’¡ ç§å­ -1 è¡¨ç¤ºéšæœºç”Ÿæˆï¼›æ‰¹é‡æ—¶è‹¥é -1ï¼Œåˆ™ä½¿ç”¨ seed+i</div>'),\n",
        ")\n",
        "\n",
        "opt_section_content = make_section(\n",
        "    \"ğŸš€ æ˜¾å­˜ä¼˜åŒ–\",\n",
        "    w_attn,\n",
        "    w_int8,\n",
        "    w_dtype_mode,\n",
        "    w_dtype_apply,\n",
        "    dtype_hint,\n",
        "    widgets.HTML('<div class=\"zimg-info\">ğŸ’¡ æ³¨æ„åŠ›åˆ‡ç‰‡é€‰ auto æ—¶å°†æŒ‰ max æ‰§è¡Œï¼ˆæ›´çœæ˜¾å­˜ï¼‰</div>'),\n",
        ")\n",
        "opt_section = make_collapsible_section(\"ğŸš€ æ˜¾å­˜ä¼˜åŒ–\", opt_section_content, open_default=True)\n",
        "\n",
        "vae_section_content = make_section(\n",
        "    \"ğŸ–¼ï¸ VAE è§£ç \",\n",
        "    w_scaling,\n",
        "    w_tiled,\n",
        "    w_tile,\n",
        "    widgets.HTML(\"<div class='zimg-info'>ğŸ’¡ ç”Ÿæˆå¤§å›¾ï¼ˆâ‰¥1536pxï¼‰å»ºè®®å¼€å¯åˆ†å—è§£ç ï¼›tile=256/384/512 æ›´ç¨³</div>\"),\n",
        ")\n",
        "vae_section = make_collapsible_section(\"ğŸ–¼ï¸ VAE è§£ç \", vae_section_content, open_default=False)\n",
        "\n",
        "save_section_content = make_section(\n",
        "    \"ğŸ’¾ ä¿å­˜è®¾ç½®\",\n",
        "    widgets.HBox([w_save_latent, w_save_png]),\n",
        "    w_latent_dir,\n",
        "    w_png_dir,\n",
        ")\n",
        "save_section = make_collapsible_section(\"ğŸ’¾ ä¿å­˜è®¾ç½®\", save_section_content, open_default=False)\n",
        "\n",
        "lora_section_content = make_section(\n",
        "    \"ğŸ­ LoRAï¼ˆ3æ§½ä½Â·æ¨ç†å‰è‡ªåŠ¨åº”ç”¨ï¼‰\",\n",
        "    widgets.HTML('''\n",
        "    <div style=\"color:#64748b;font-size:12px;padding:8px;background:#0f172a;border-radius:8px;\">\n",
        "        ğŸ“‹ æ§½ä½æ˜¾ç¤º LoRA æ–‡ä»¶åï¼ˆé¿å…è¯¯ç”¨ï¼‰ï¼›æ¨ç†å‰ä¼šæŒ‰æ§½ä½çŠ¶æ€è‡ªåŠ¨åŠ è½½/å¸è½½å¹¶è®¾ç½®æƒé‡ï¼ˆè‹¥pipeæ”¯æŒï¼‰ã€‚\n",
        "    </div>\n",
        "    '''),\n",
        "    lora_slots_ui,\n",
        ")\n",
        "lora_section = make_collapsible_section(\"ğŸ­ LoRAï¼ˆ3æ§½ä½Â·æ¨ç†å‰è‡ªåŠ¨åº”ç”¨ï¼‰\", lora_section_content, open_default=False)\n",
        "\n",
        "svh_section_content = make_section(\n",
        "    \"ğŸ§¬ SeedVarianceEnhancer\",\n",
        "    widgets.HTML(\"\"\"\n",
        "    <div style=\"color:#64748b;font-size:12px;padding:8px;background:#0f172a;border-radius:8px;\">\n",
        "      è¯´æ˜ï¼šæ­¤å¤„åœ¨ <b>TE ç¼–ç å</b> å¯¹ <code>prompt_embeds</code>ï¼ˆä»¥åŠ CFG&gt;1 æ—¶çš„ <code>negative_prompt_embeds</code>ï¼‰åšå™ªå£°æ³¨å…¥ï¼Œå¢å¼ºå¤šæ ·æ€§ã€‚<br>\n",
        "      âš ï¸ Colab pipeline æ¨¡å¼ä¸æ”¯æŒ ComfyUI conditioning çš„\"æŒ‰æ­¥ begin/end åˆ‡æ¢\"ï¼Œå› æ­¤ begin/end ä¼šæŒ‰ all steps ç­‰ä»·å¤„ç†ï¼ˆå‚æ•°ä»ä¿ç•™ä»¥ä¾¿è®°å½•/è¿ç§»ï¼‰ã€‚\n",
        "    </div>\n",
        "    \"\"\"),\n",
        "    w_svh_enable,\n",
        "    w_svh_randomize_percent,\n",
        "    w_svh_strength,\n",
        "    w_svh_noise_insert,\n",
        "    w_svh_steps_switchover_percent,\n",
        "    w_svh_mask_starts_at,\n",
        "    w_svh_mask_percent,\n",
        "    w_svh_seed_mode,\n",
        "    w_svh_seed,\n",
        "    w_svh_log_to_console,\n",
        ")\n",
        "svh_section = make_collapsible_section(\"ğŸ§¬ SeedVarianceEnhancerï¼ˆå¤šæ ·æ€§/ç§å­æ‰°åŠ¨ï¼‰\", svh_section_content, open_default=False)\n",
        "\n",
        "# ==================== æŒ‰é’®åŒºï¼šåœ¨ç”ŸæˆæŒ‰é’®æ—åŠ å…¥\"æ¯æ¬¡å¼ æ•°\" ====================\n",
        "btn_run.on_click(_on_run)\n",
        "btn_clear.on_click(_on_clear)\n",
        "\n",
        "button_section = widgets.HBox(\n",
        "    [btn_run, w_batch, btn_clear],\n",
        "    layout=widgets.Layout(justify_content=\"center\", margin=\"16px 0\", gap=\"16px\", align_items=\"center\")\n",
        ")\n",
        "\n",
        "main_panel = widgets.VBox([\n",
        "    header_html,\n",
        "    allocator_html,\n",
        "    prompt_section,\n",
        "    size_section,\n",
        "    gen_section,\n",
        "    opt_section,\n",
        "    svh_section,\n",
        "    vae_section,\n",
        "    save_section,\n",
        "    lora_section,\n",
        "    button_section,\n",
        "    widgets.HTML('<div class=\"zimg-section-title\" style=\"margin-top:16px;\">ğŸ“º è¾“å‡ºåŒºåŸŸ</div>'),\n",
        "    out,\n",
        "], layout=widgets.Layout(\n",
        "    width=\"800px\",\n",
        "    padding=\"20px\",\n",
        "    border_radius=\"16px\",\n",
        "    background=\"#0f172a\",\n",
        "))\n",
        "\n",
        "display(main_panel)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IBC5UBZ0XrnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ§© Latent è§£ç æ¨¡å—(ä½¿ç”¨ä¸åŒVAEè§£ç ï¼Œä¸æ‡‚å°±ä¸ç”¨)\n",
        "import os, re, gc, json, zipfile, tarfile, hashlib, traceback\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image, PngImagePlugin\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from datetime import datetime\n",
        "\n",
        "OUT_DIR_DEFAULT = \"/content/drive/MyDrive/Z-image/VAE-image\"\n",
        "os.makedirs(OUT_DIR_DEFAULT, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# utils\n",
        "# -------------------------\n",
        "def _hard_cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        try:\n",
        "            torch.cuda.ipc_collect()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def _safe_str(v, max_len=8000):\n",
        "    try:\n",
        "        s = json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list, tuple)) else str(v)\n",
        "    except Exception:\n",
        "        s = str(v)\n",
        "    if max_len is not None and len(s) > max_len:\n",
        "        s = s[:max_len] + \"...(trunc)\"\n",
        "    return s\n",
        "\n",
        "def _sha256(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _ensure_parent_dir(fp: str):\n",
        "    d = os.path.dirname(fp)\n",
        "    if d:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def _print_gpu_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
        "        resv = torch.cuda.memory_reserved() / (1024**3)\n",
        "        print(f\"ğŸ’¾ GPU allocated={alloc:.2f} GB | reserved={resv:.2f} GB\")\n",
        "    else:\n",
        "        print(\"ğŸ’¾ CUDA not available (CPU runtime)\")\n",
        "\n",
        "# è®©æ—¥å¿—æ—¢è¿› outï¼Œåˆè¿› stdoutï¼ˆä½ åœ¨å“ªçœ‹éƒ½èƒ½çœ‹åˆ°ï¼‰\n",
        "def _log(msg: str):\n",
        "    print(msg)\n",
        "\n",
        "# -------------------------\n",
        "# latent load (npz / npy / dict-wrapped)\n",
        "# -------------------------\n",
        "def _unwrap_object_array(x, max_depth=10):\n",
        "    cur = x\n",
        "    for _ in range(max_depth):\n",
        "        if isinstance(cur, np.ndarray) and cur.dtype == np.object_:\n",
        "            if cur.size == 1:\n",
        "                cur = cur.reshape(-1)[0]\n",
        "                continue\n",
        "            return cur\n",
        "        if isinstance(cur, np.generic):\n",
        "            try:\n",
        "                cur2 = cur.item()\n",
        "                if cur2 is cur:\n",
        "                    return cur\n",
        "                cur = cur2\n",
        "                continue\n",
        "            except Exception:\n",
        "                return cur\n",
        "        return cur\n",
        "    return cur\n",
        "\n",
        "def _to_numeric_ndarray(x, prefer_dtype=np.float32):\n",
        "    x = _unwrap_object_array(x)\n",
        "\n",
        "    if isinstance(x, np.ndarray) and x.dtype != np.object_:\n",
        "        if prefer_dtype is not None and x.dtype != prefer_dtype:\n",
        "            if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.integer):\n",
        "                x = x.astype(prefer_dtype, copy=False)\n",
        "        return x\n",
        "\n",
        "    if torch.is_tensor(x):\n",
        "        x = x.detach().cpu().numpy()\n",
        "        if prefer_dtype is not None and x.dtype != prefer_dtype:\n",
        "            if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.integer):\n",
        "                x = x.astype(prefer_dtype, copy=False)\n",
        "        return x\n",
        "\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        arr = np.array(x)\n",
        "        arr = _unwrap_object_array(arr)\n",
        "        if isinstance(arr, np.ndarray) and arr.dtype != np.object_:\n",
        "            if prefer_dtype is not None and arr.dtype != prefer_dtype:\n",
        "                if np.issubdtype(arr.dtype, np.floating) or np.issubdtype(arr.dtype, np.integer):\n",
        "                    arr = arr.astype(prefer_dtype, copy=False)\n",
        "            return arr\n",
        "\n",
        "    raise TypeError(\n",
        "        \"latent æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ ndarrayã€‚\\n\"\n",
        "        f\"- type={type(x)}\\n\"\n",
        "        f\"- repr={_safe_str(x, max_len=600)}\"\n",
        "    )\n",
        "\n",
        "def _merge_meta_from_dict(d: dict):\n",
        "    meta = {}\n",
        "    for k, v in d.items():\n",
        "        if k == \"latent\":\n",
        "            continue\n",
        "        try:\n",
        "            if isinstance(v, np.generic):\n",
        "                v = v.item()\n",
        "        except Exception:\n",
        "            pass\n",
        "        meta[str(k)] = v\n",
        "    return meta\n",
        "\n",
        "def load_latent_any(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise RuntimeError(\"latent è·¯å¾„ä¸å­˜åœ¨: \" + path)\n",
        "\n",
        "    lower = path.lower()\n",
        "\n",
        "    if lower.endswith(\".npy\"):\n",
        "        raw = np.load(path, allow_pickle=True)\n",
        "        raw = _unwrap_object_array(raw)\n",
        "        if isinstance(raw, dict):\n",
        "            if \"latent\" not in raw:\n",
        "                raise RuntimeError(\"è¯¥ .npy(dict) å†…æ²¡æœ‰ 'latent' é”®ï¼Œå¯ç”¨ keys=\" + \", \".join(map(str, raw.keys())))\n",
        "            latent = _to_numeric_ndarray(raw[\"latent\"], prefer_dtype=np.float32)\n",
        "            meta = _merge_meta_from_dict(raw)\n",
        "            return latent, meta\n",
        "        latent = _to_numeric_ndarray(raw, prefer_dtype=np.float32)\n",
        "        return latent, {}\n",
        "\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    data = _unwrap_object_array(data)\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        if \"latent\" not in data:\n",
        "            raise RuntimeError(\"è¯¥æ–‡ä»¶(dict)å†…æ²¡æœ‰ 'latent' é”®ï¼Œå¯ç”¨ keys=\" + \", \".join(map(str, data.keys())))\n",
        "        latent = _to_numeric_ndarray(data[\"latent\"], prefer_dtype=np.float32)\n",
        "        meta = _merge_meta_from_dict(data)\n",
        "        return latent, meta\n",
        "\n",
        "    if isinstance(data, np.ndarray):\n",
        "        latent = _to_numeric_ndarray(data, prefer_dtype=np.float32)\n",
        "        return latent, {}\n",
        "\n",
        "    if not hasattr(data, \"files\"):\n",
        "        raise RuntimeError(f\"æ— æ³•è§£æè¯¥æ–‡ä»¶ä¸º npz/npy/dictï¼šnp.load ç±»å‹={type(data)}\")\n",
        "\n",
        "    if \"latent\" not in data.files:\n",
        "        raise RuntimeError(f\"npz å†…æ‰¾ä¸åˆ° key=latentï¼Œå¯ç”¨ keys={data.files}\")\n",
        "\n",
        "    latent = _to_numeric_ndarray(data[\"latent\"], prefer_dtype=np.float32)\n",
        "\n",
        "    meta = {}\n",
        "    if \"metadata_json\" in data.files:\n",
        "        try:\n",
        "            meta_json = data[\"metadata_json\"].item()\n",
        "            meta = json.loads(meta_json) if isinstance(meta_json, str) else json.loads(str(meta_json))\n",
        "        except Exception:\n",
        "            meta = {\"metadata_json_raw\": _safe_str(data[\"metadata_json\"].item())}\n",
        "\n",
        "    for k in data.files:\n",
        "        if k.startswith(\"meta_\"):\n",
        "            key = k[5:]\n",
        "            vv = data[k]\n",
        "            try:\n",
        "                vv = vv.item() if hasattr(vv, \"item\") else vv\n",
        "            except Exception:\n",
        "                pass\n",
        "            meta[key] = vv\n",
        "\n",
        "    return latent, meta\n",
        "\n",
        "# -------------------------\n",
        "# PNG write meta\n",
        "# -------------------------\n",
        "def save_png_with_all_meta(path: str, img: Image.Image, meta: dict):\n",
        "    pnginfo = PngImagePlugin.PngInfo()\n",
        "\n",
        "    prompt = (meta.get(\"prompt\", \"\") or \"\")\n",
        "    negative = (meta.get(\"negative_prompt\", \"\") or \"\")\n",
        "    parameters = prompt\n",
        "    if negative.strip():\n",
        "        parameters += \"\\nNegative prompt: \" + negative\n",
        "    pnginfo.add_text(\"parameters\", parameters)\n",
        "\n",
        "    pnginfo.add_text(\"zimg_meta_json\", json.dumps(meta, ensure_ascii=False, sort_keys=True))\n",
        "\n",
        "    for k, v in meta.items():\n",
        "        try:\n",
        "            pnginfo.add_text(str(k), _safe_str(v, max_len=8000))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    _ensure_parent_dir(path)\n",
        "    img.save(path, pnginfo=pnginfo)\n",
        "\n",
        "# -------------------------\n",
        "# VAE download / resolve / load\n",
        "# -------------------------\n",
        "def download_url_to_file(url: str, dst_file: str):\n",
        "    url = (url or \"\").strip()\n",
        "    if not re.match(r\"^https?://\", url, re.I):\n",
        "        raise RuntimeError(\"URL æ— æ•ˆ: \" + url)\n",
        "    dst_file = (dst_file or \"\").strip()\n",
        "    if not dst_file:\n",
        "        raise RuntimeError(\"è¯·å¡«å†™ä¸‹è½½ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå»ºè®®æ”¾Driveï¼‰\")\n",
        "    _ensure_parent_dir(dst_file)\n",
        "\n",
        "    if os.path.exists(dst_file) and os.path.getsize(dst_file) > 0:\n",
        "        _log(\"â„¹ï¸ å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: \" + dst_file)\n",
        "        return dst_file\n",
        "\n",
        "    import urllib.request\n",
        "    _log(\"ğŸŒ downloading: \" + url)\n",
        "    urllib.request.urlretrieve(url, dst_file)\n",
        "    _log(\"âœ… downloaded: \" + dst_file + \" sha256:\" + _sha256(dst_file)[:16] + \"...\")\n",
        "    return dst_file\n",
        "\n",
        "def extract_archive_if_needed(src_path: str, dst_dir: str):\n",
        "    lower = src_path.lower()\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "    if lower.endswith(\".zip\"):\n",
        "        _log(\"ğŸ“¦ extracting zip -> \" + dst_dir)\n",
        "        with zipfile.ZipFile(src_path, \"r\") as z:\n",
        "            z.extractall(dst_dir)\n",
        "        return dst_dir\n",
        "\n",
        "    if lower.endswith(\".tar\") or lower.endswith(\".tar.gz\") or lower.endswith(\".tgz\"):\n",
        "        _log(\"ğŸ“¦ extracting tar -> \" + dst_dir)\n",
        "        with tarfile.open(src_path, \"r:*\") as t:\n",
        "            t.extractall(dst_dir)\n",
        "        return dst_dir\n",
        "\n",
        "    return src_path if os.path.isdir(src_path) else os.path.dirname(src_path)\n",
        "\n",
        "def resolve_vae_dir(root_dir: str):\n",
        "    if os.path.isdir(root_dir) and os.path.exists(os.path.join(root_dir, \"config.json\")) and (\n",
        "        os.path.exists(os.path.join(root_dir, \"diffusion_pytorch_model.safetensors\")) or\n",
        "        os.path.exists(os.path.join(root_dir, \"diffusion_pytorch_model.bin\"))\n",
        "    ):\n",
        "        return root_dir\n",
        "\n",
        "    cand = os.path.join(root_dir, \"vae\")\n",
        "    if os.path.isdir(cand) and os.path.exists(os.path.join(cand, \"config.json\")):\n",
        "        return cand\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        if \"config.json\" in filenames and (\n",
        "            \"diffusion_pytorch_model.safetensors\" in filenames or \"diffusion_pytorch_model.bin\" in filenames\n",
        "        ):\n",
        "            if os.path.basename(dirpath).lower() == \"vae\":\n",
        "                return dirpath\n",
        "\n",
        "    raise RuntimeError(f\"åœ¨ {root_dir} ä¸‹æ‰¾ä¸åˆ°å¯ç”¨çš„ VAE ç›®å½•ï¼ˆéœ€è¦ config.json + æƒé‡æ–‡ä»¶ï¼‰\")\n",
        "\n",
        "def load_vae(vae_dir: str, device=\"cuda\"):\n",
        "    from diffusers import AutoencoderKL\n",
        "\n",
        "    has_cuda = torch.cuda.is_available()\n",
        "    if device == \"cuda\" and not has_cuda:\n",
        "        _log(\"âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼Œè‡ªåŠ¨æ”¹ç”¨ CPU è§£ç \")\n",
        "        device = \"cpu\"\n",
        "\n",
        "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    _log(f\"ğŸ”§ load_vae: device={device} dtype={dtype}\")\n",
        "\n",
        "    vae = AutoencoderKL.from_pretrained(vae_dir, torch_dtype=dtype, local_files_only=True)\n",
        "    vae.eval()\n",
        "    vae.to(device)\n",
        "    return vae\n",
        "\n",
        "# -------------------------\n",
        "# decode\n",
        "# -------------------------\n",
        "def decode_latent_with_vae(vae, latent_np: np.ndarray, scaling_factor: float):\n",
        "    if not isinstance(latent_np, np.ndarray):\n",
        "        raise TypeError(f\"latent ä¸æ˜¯ ndarrayï¼š{type(latent_np)}\")\n",
        "    if latent_np.dtype == np.object_:\n",
        "        raise TypeError(f\"latent dtype=objectï¼ˆä»æœªè§£åŒ…æˆåŠŸï¼‰ã€‚shape={latent_np.shape}\")\n",
        "\n",
        "    lat = torch.from_numpy(latent_np)\n",
        "    if lat.ndim == 3:\n",
        "        lat = lat.unsqueeze(0)\n",
        "    if lat.ndim != 4:\n",
        "        raise RuntimeError(f\"latent ç»´åº¦ä¸å¯¹ï¼š{tuple(lat.shape)}ï¼ˆæœŸæœ› NCHWï¼‰\")\n",
        "\n",
        "    device = next(vae.parameters()).device\n",
        "    dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "    lat = lat.to(device=device, dtype=dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded = vae.decode(lat / float(scaling_factor)).sample\n",
        "\n",
        "    img = (decoded[0] / 2 + 0.5).clamp(0, 1).detach().cpu().permute(1, 2, 0).float().numpy()\n",
        "    return Image.fromarray((img * 255).astype(np.uint8))\n",
        "\n",
        "# ======================================================\n",
        "# UI\n",
        "# ======================================================\n",
        "_state = {\"latent\": None, \"meta\": None, \"img\": None, \"vae_dir\": None, \"vae\": None}\n",
        "\n",
        "latent_path = widgets.Text(value=\"\", description=\"latent\", layout=widgets.Layout(width=\"99%\"))\n",
        "\n",
        "w_vae_load_path = widgets.Text(\n",
        "    value=\"\",\n",
        "    description=\"åŠ è½½è·¯å¾„\",\n",
        "    placeholder=\"æœ¬åœ° VAE ç›®å½• / æ¨¡å‹æ ¹ç›®å½• / å‹ç¼©åŒ…æ–‡ä»¶(.zip/.tar/.tgz)\",\n",
        "    layout=widgets.Layout(width=\"99%\"),\n",
        ")\n",
        "w_extract_dir = widgets.Text(value=\"/content/vae_extract\", description=\"è§£å‹åˆ°\", layout=widgets.Layout(width=\"99%\"))\n",
        "\n",
        "w_device = widgets.Dropdown(options=[\"cuda\", \"cpu\"], value=\"cpu\", description=\"device\", layout=widgets.Layout(width=\"220px\"))\n",
        "w_scaling = widgets.FloatText(value=0.3611, description=\"scale\", layout=widgets.Layout(width=\"220px\"))\n",
        "\n",
        "w_out_dir = widgets.Text(value=OUT_DIR_DEFAULT, description=\"out_dir\", layout=widgets.Layout(width=\"99%\"))\n",
        "w_auto_name = widgets.Checkbox(value=True, description=\"è‡ªåŠ¨å‘½åè¾“å‡ºæ–‡ä»¶\", indent=False)\n",
        "w_out_name = widgets.Text(value=\"vae_decode.png\", description=\"æ–‡ä»¶å\", layout=widgets.Layout(width=\"99%\"))\n",
        "\n",
        "btn_load_vae = widgets.Button(description=\"0) åŠ è½½VAE\", button_style=\"\")\n",
        "btn_unload_vae = widgets.Button(description=\"å¸è½½VAE\", button_style=\"warning\")\n",
        "btn_decode = widgets.Button(description=\"1) è§£ç é¢„è§ˆ\", button_style=\"\")\n",
        "btn_save = widgets.Button(description=\"2) ä¿å­˜PNGï¼ˆå«å…¨éƒ¨metaï¼‰\", button_style=\"success\")\n",
        "btn_clear = widgets.Button(description=\"æ¸…ç©ºè¾“å‡º\", button_style=\"\")\n",
        "\n",
        "status = widgets.HTML(\"<div style='color:#64748b'>çŠ¶æ€ï¼šå°±ç»ª</div>\")\n",
        "out = widgets.Output(layout=widgets.Layout(width=\"980px\", border=\"1px solid #334155\", padding=\"10px\"))\n",
        "\n",
        "def _set_status(s: str):\n",
        "    status.value = f\"<div style='color:#64748b'><b>çŠ¶æ€ï¼š</b>{s}</div>\"\n",
        "\n",
        "def _unload_current_vae():\n",
        "    vae = _state.get(\"vae\", None)\n",
        "    _state[\"vae\"] = None\n",
        "    _state[\"vae_dir\"] = None\n",
        "    if vae is not None:\n",
        "        try:\n",
        "            vae.to(\"cpu\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        del vae\n",
        "    _hard_cleanup()\n",
        "\n",
        "def on_unload_vae(_):\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        _set_status(\"å¸è½½VAEä¸­â€¦\")\n",
        "        print(\"[unload] clicked\")\n",
        "        _unload_current_vae()\n",
        "        print(\"âœ… VAE unloaded\")\n",
        "        _print_gpu_mem()\n",
        "        _set_status(\"VAEå·²å¸è½½\")\n",
        "\n",
        "def on_load_vae(_):\n",
        "    # åŒé€šé“ï¼šstdout + out\n",
        "    print(\"[load] clicked\")  # stdout\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        try:\n",
        "            _set_status(\"åŠ è½½VAEä¸­â€¦\")\n",
        "            print(\"[load] clicked (in out)\")\n",
        "\n",
        "            if _state.get(\"vae\", None) is not None:\n",
        "                print(\"â„¹ï¸ å·²æœ‰VAEï¼Œå…ˆå¸è½½æ—§VAEâ€¦\")\n",
        "                _unload_current_vae()\n",
        "\n",
        "            lp = (w_vae_load_path.value or \"\").strip()\n",
        "            print(\"[load] load_path:\", lp)\n",
        "            if not lp:\n",
        "                raise RuntimeError(\"åŠ è½½è·¯å¾„ä¸ºç©ºï¼šè¯·å¡« VAE ç›®å½•/æ¨¡å‹æ ¹/å‹ç¼©åŒ…æ–‡ä»¶è·¯å¾„\")\n",
        "            if not os.path.exists(lp):\n",
        "                raise RuntimeError(\"åŠ è½½è·¯å¾„ä¸å­˜åœ¨: \" + lp)\n",
        "\n",
        "            root = lp\n",
        "            if os.path.isfile(lp) and re.search(r\"\\.(zip|tar|tar\\.gz|tgz)$\", lp, re.I):\n",
        "                ed = (w_extract_dir.value.strip() or \"/content/vae_extract\")\n",
        "                print(\"[load] extracting to:\", ed)\n",
        "                root = extract_archive_if_needed(lp, ed)\n",
        "\n",
        "            print(\"[load] resolve_vae_dir root:\", root)\n",
        "            vae_dir = resolve_vae_dir(root)\n",
        "            _state[\"vae_dir\"] = vae_dir\n",
        "            print(\"ğŸ“ resolved vae_dir:\", vae_dir)\n",
        "\n",
        "            print(\"[load] load_vaeâ€¦ device:\", w_device.value)\n",
        "            vae = load_vae(vae_dir, device=w_device.value)\n",
        "            _state[\"vae\"] = vae\n",
        "            print(\"âœ… VAE loaded:\", vae.__class__.__name__)\n",
        "            _print_gpu_mem()\n",
        "            _set_status(\"VAEå·²åŠ è½½\")\n",
        "        except Exception:\n",
        "            _set_status(\"åŠ è½½VAEå¤±è´¥ï¼ˆè§ä¸‹æ–¹ï¼‰\")\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            _hard_cleanup()\n",
        "\n",
        "def on_decode(_):\n",
        "    print(\"[decode] clicked\")\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        try:\n",
        "            _set_status(\"è§£ç ä¸­â€¦\")\n",
        "            if _state.get(\"vae\", None) is None:\n",
        "                raise RuntimeError(\"è¯·å…ˆç‚¹ã€Œ0) åŠ è½½VAEã€\")\n",
        "\n",
        "            lp = latent_path.value.strip()\n",
        "            print(\"[decode] latent_path:\", lp)\n",
        "            latent, meta = load_latent_any(lp)\n",
        "            _state[\"latent\"] = latent\n",
        "            _state[\"meta\"] = meta\n",
        "\n",
        "            if \"vae_scaling_factor\" in meta:\n",
        "                try:\n",
        "                    w_scaling.value = float(meta[\"vae_scaling_factor\"])\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            img = decode_latent_with_vae(_state[\"vae\"], latent, scaling_factor=float(w_scaling.value))\n",
        "            _state[\"img\"] = img\n",
        "            display(img)\n",
        "            print(\"âœ… decoded | img.size:\", img.size)\n",
        "            print(\"latent:\", latent.shape, latent.dtype)\n",
        "            print(\"meta keys:\", len(meta))\n",
        "            _set_status(\"è§£ç å®Œæˆ\")\n",
        "        except Exception:\n",
        "            _set_status(\"è§£ç å¤±è´¥ï¼ˆè§ä¸‹æ–¹ï¼‰\")\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            _hard_cleanup()\n",
        "\n",
        "def on_save(_):\n",
        "    print(\"[save] clicked\")\n",
        "    out.clear_output()\n",
        "    with out:\n",
        "        try:\n",
        "            _set_status(\"ä¿å­˜ä¸­â€¦\")\n",
        "            if _state.get(\"img\", None) is None or _state.get(\"meta\", None) is None:\n",
        "                raise RuntimeError(\"è¯·å…ˆç‚¹ã€Œ1) è§£ç é¢„è§ˆã€\")\n",
        "\n",
        "            out_dir = (w_out_dir.value or OUT_DIR_DEFAULT).strip()\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "            meta = dict(_state[\"meta\"])\n",
        "            meta[\"vae_decode\"] = {\n",
        "                \"vae_load_path\": (w_vae_load_path.value or \"\").strip(),\n",
        "                \"vae_dir_resolved\": _state.get(\"vae_dir\"),\n",
        "                \"scaling_factor\": float(w_scaling.value),\n",
        "                \"device\": w_device.value,\n",
        "                \"decoded_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"latent_path\": latent_path.value.strip(),\n",
        "            }\n",
        "\n",
        "            if bool(w_auto_name.value):\n",
        "                base = os.path.splitext(os.path.basename(latent_path.value.strip()))[0]\n",
        "                out_name = f\"vae_decode_{base}.png\"\n",
        "            else:\n",
        "                out_name = (w_out_name.value or \"vae_decode.png\").strip()\n",
        "\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "            save_png_with_all_meta(out_path, _state[\"img\"], meta)\n",
        "            print(\"ğŸ’¾ saved:\", out_path)\n",
        "            _set_status(\"ä¿å­˜å®Œæˆ\")\n",
        "        except Exception:\n",
        "            _set_status(\"ä¿å­˜å¤±è´¥ï¼ˆè§ä¸‹æ–¹ï¼‰\")\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            _hard_cleanup()\n",
        "\n",
        "def on_clear(_):\n",
        "    out.clear_output()\n",
        "    _set_status(\"å°±ç»ª\")\n",
        "\n",
        "btn_load_vae.on_click(on_load_vae)\n",
        "btn_unload_vae.on_click(on_unload_vae)\n",
        "btn_decode.on_click(on_decode)\n",
        "btn_save.on_click(on_save)\n",
        "btn_clear.on_click(on_clear)\n",
        "\n",
        "ui = widgets.VBox(\n",
        "    [\n",
        "        widgets.HTML(\"<b>CPU/GPU é€šç”¨ Latent è§£ç </b><div style='color:#64748b;font-size:12px'>\"\n",
        "                     \"å·²åŠ å…¥åŒé€šé“æ—¥å¿—ï¼šæŒ‰é’®å›è°ƒä¸€å®šä¼šåœ¨ stdout å’Œ out åŒºæ˜¾ç¤º clickedï¼Œé¿å…â€œçœ‹èµ·æ¥æ²¡ååº”â€ã€‚</div>\"),\n",
        "        widgets.HTML(\"<hr><b>â‘  latent</b>\"),\n",
        "        latent_path,\n",
        "        widgets.HTML(\"<hr><b>â‘¡ VAE åŠ è½½</b>\"),\n",
        "        w_vae_load_path,\n",
        "        w_extract_dir,\n",
        "        widgets.HBox([w_device, w_scaling]),\n",
        "        widgets.HBox([btn_load_vae, btn_unload_vae, btn_decode, btn_save, btn_clear]),\n",
        "        status,\n",
        "        widgets.HTML(\"<hr><b>â‘¢ è¾“å‡º</b>\"),\n",
        "        w_out_dir,\n",
        "        widgets.HBox([w_auto_name]),\n",
        "        w_out_name,\n",
        "        out,\n",
        "    ],\n",
        "    layout=widgets.Layout(width=\"980px\"),\n",
        ")\n",
        "\n",
        "display(ui)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eIbszS-qYk-f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}