{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title z-image-base - INT8 Per-Channel é‡åŒ–ï¼ˆTurbo å®Œå…¨å¯¹é½ç‰ˆï¼‰\n",
        "\n",
        "# ==========================================\n",
        "# æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–\n",
        "# ==========================================\n",
        "!pip install -U -q git+https://github.com/Disty0/sdnq git+https://github.com/huggingface/diffusers\n",
        "!pip install -U -q transformers accelerate safetensors psutil\n",
        "\n",
        "# ==========================================\n",
        "# æ­¥éª¤ 2ï¼šä¸»é‡åŒ–æµç¨‹\n",
        "# ==========================================\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "import shutil\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from safetensors.torch import save_file, load_file\n",
        "from huggingface_hub import hf_hub_download\n",
        "import psutil\n",
        "\n",
        "def print_mem():\n",
        "    mem = psutil.virtual_memory()\n",
        "    used_gb = mem.used / 1e9\n",
        "    total_gb = mem.total / 1e9\n",
        "    percent = mem.percent\n",
        "    print(f\"ğŸ’» RAM: {used_gb:.1f}GB / {total_gb:.1f}GB ({percent:.1f}%)\")\n",
        "    if percent > 85:\n",
        "        print(\"âš ï¸  å†…å­˜ä½¿ç”¨è¶…è¿‡ 85%ï¼Œå¼ºåˆ¶ GC\")\n",
        "        gc.collect()\n",
        "\n",
        "def force_cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# æŒ‚è½½ Drive\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "FINAL_DIR = \"/content/drive/MyDrive/z-image-base-transformer-int8-per-channel\"\n",
        "LOCAL_DIR = \"/content/quant_output\"\n",
        "TEMP_DIR = \"/content/temp_quant\"\n",
        "\n",
        "for d in [LOCAL_DIR, TEMP_DIR]:\n",
        "    shutil.rmtree(d, ignore_errors=True)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# ä¸ Turbo å®Œå…¨ä¸€è‡´çš„æ’é™¤åˆ—è¡¨\n",
        "target_modules_to_not_convert = [\n",
        "    \"layers.0.adaLN_modulation.0.weight\",\n",
        "    \"all_x_embedder\",\n",
        "    \"t_embedder\",\n",
        "    \"cap_embedder\",\n",
        "    \"all_final_layer\"\n",
        "]\n",
        "\n",
        "def should_quantize(name):\n",
        "    \"\"\"åˆ¤æ–­æ˜¯å¦é‡åŒ–ï¼ˆä¸ Turbo é€»è¾‘ä¸€è‡´ï¼‰\"\"\"\n",
        "    if name in target_modules_to_not_convert:\n",
        "        return False\n",
        "\n",
        "    for skip_prefix in target_modules_to_not_convert:\n",
        "        if name.startswith(skip_prefix):\n",
        "            return False\n",
        "\n",
        "    return name.endswith(\".weight\")\n",
        "\n",
        "def quantize_tensor_int8_per_channel(tensor):\n",
        "    \"\"\"\n",
        "    INT8 Per-Channel å¯¹ç§°é‡åŒ–ï¼ˆä¸ Turbo å®Œå…¨å¯¹é½ï¼‰\n",
        "\n",
        "    å¯¹äº 2D æƒé‡ [out_features, in_features]ï¼š\n",
        "    - å¯¹æ¯ä¸ªè¾“å‡ºé€šé“ï¼ˆdim=0ï¼‰ç‹¬ç«‹è®¡ç®— scale\n",
        "    - è¿”å› scale shape: [out_features, 1]ï¼ˆä¸ Turbo ä¸€è‡´ï¼‰\n",
        "\n",
        "    å¯¹äº 1D æƒé‡ï¼ˆå¦‚ bias/normï¼‰ï¼š\n",
        "    - ä½¿ç”¨ per-tensor é‡åŒ–\n",
        "    \"\"\"\n",
        "    if tensor.dtype == torch.int8:\n",
        "        return tensor, None\n",
        "\n",
        "    t_float = tensor.float()\n",
        "\n",
        "    # ğŸ”§ å…³é”®ï¼š2D æƒé‡ç”¨ per-channelï¼Œ1D ç”¨ per-tensor\n",
        "    if tensor.dim() == 2:\n",
        "        # Per-Channel é‡åŒ–\n",
        "        # æ²¿ dim=1 æ±‚æœ€å¤§å€¼ï¼Œä¿ç•™ dim=0ï¼ˆè¾“å‡ºé€šé“ï¼‰\n",
        "        abs_max_per_channel = t_float.abs().amax(dim=1, keepdim=True)  # shape: [out_features, 1]\n",
        "\n",
        "        # é¿å…é™¤é›¶\n",
        "        abs_max_per_channel = torch.clamp(abs_max_per_channel, min=1e-8)\n",
        "\n",
        "        # è®¡ç®— scaleï¼ˆæ¯ä¸ªè¾“å‡ºé€šé“ä¸€ä¸ªï¼‰\n",
        "        scale = abs_max_per_channel / 127.0  # shape: [out_features, 1]\n",
        "\n",
        "        # é‡åŒ–ï¼ˆè‡ªåŠ¨å¹¿æ’­ï¼‰\n",
        "        q_tensor = (t_float / scale).round().clamp(-128, 127).to(torch.int8)\n",
        "\n",
        "        # ğŸ”§ å…³é”®ï¼šä¿æŒ scale çš„ shape ä¸º [out_features, 1]ï¼ˆä¸ Turbo ä¸€è‡´ï¼‰\n",
        "        return q_tensor, scale.to(torch.bfloat16)\n",
        "\n",
        "    else:\n",
        "        # Per-Tensor é‡åŒ–ï¼ˆç”¨äº 1D/3D+ æƒé‡ï¼‰\n",
        "        abs_max = t_float.abs().max().item()\n",
        "\n",
        "        if abs_max == 0 or abs_max < 1e-8:\n",
        "            return tensor.to(torch.int8), torch.tensor(1.0, dtype=torch.bfloat16)\n",
        "\n",
        "        scale = abs_max / 127.0\n",
        "        q_tensor = (t_float / scale).round().clamp(-128, 127).to(torch.int8)\n",
        "\n",
        "        # ğŸ”§ è¿”å›æ ‡é‡ scale\n",
        "        return q_tensor, torch.tensor(scale, dtype=torch.bfloat16)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸš€ å¼€å§‹ Per-Channel é‡åŒ–æµç¨‹ï¼ˆTurbo å®Œå…¨å¯¹é½ï¼‰\")\n",
        "print(\"=\"*60)\n",
        "print_mem()\n",
        "\n",
        "# ==========================================\n",
        "# 1. ä¸‹è½½åŸå§‹æ¨¡å‹\n",
        "# ==========================================\n",
        "print(\"\\nğŸ“¥ æ­¥éª¤ 1: ä¸‹è½½æ¨¡å‹ç´¢å¼•...\")\n",
        "\n",
        "index_file = hf_hub_download(\n",
        "    \"Tongyi-MAI/Z-Image\",\n",
        "    \"transformer/diffusion_pytorch_model.safetensors.index.json\",\n",
        "    cache_dir=TEMP_DIR,\n",
        "    resume_download=True\n",
        ")\n",
        "\n",
        "with open(index_file, \"r\") as f:\n",
        "    original_index = json.load(f)\n",
        "\n",
        "weight_map = original_index[\"weight_map\"]\n",
        "shard_files = sorted(set(weight_map.values()))\n",
        "\n",
        "print(f\"   âœ“ åŸå§‹åˆ†ç‰‡æ•°: {len(shard_files)}\")\n",
        "print(f\"   âœ“ æ€»å‚æ•°æ•°: {len(weight_map)}\")\n",
        "\n",
        "config_file = hf_hub_download(\n",
        "    \"Tongyi-MAI/Z-Image\",\n",
        "    \"transformer/config.json\",\n",
        "    cache_dir=TEMP_DIR,\n",
        "    resume_download=True\n",
        ")\n",
        "\n",
        "with open(config_file, \"r\") as f:\n",
        "    base_config = json.load(f)\n",
        "\n",
        "print(f\"   âœ“ config.json å·²ä¸‹è½½\")\n",
        "print_mem()\n",
        "\n",
        "# ==========================================\n",
        "# 2. é€åˆ†ç‰‡é‡åŒ–\n",
        "# ==========================================\n",
        "print(f\"\\nğŸ”„ æ­¥éª¤ 2: é€åˆ†ç‰‡é‡åŒ–...\")\n",
        "\n",
        "TEMP_SHARD_SIZE = 500_000_000  # 500MB\n",
        "\n",
        "temp_shard_idx = 1\n",
        "current_temp_shard = {}\n",
        "current_temp_size = 0\n",
        "scales_dict = {}\n",
        "\n",
        "quantized_count = 0\n",
        "kept_count = 0\n",
        "\n",
        "for shard_idx, shard_name in enumerate(shard_files):\n",
        "    print(f\"\\n{'â”€'*60}\")\n",
        "    print(f\"ğŸ“¦ åˆ†ç‰‡ {shard_idx+1}/{len(shard_files)}: {shard_name}\")\n",
        "    print_mem()\n",
        "\n",
        "    shard_path = hf_hub_download(\n",
        "        \"Tongyi-MAI/Z-Image\",\n",
        "        f\"transformer/{shard_name}\",\n",
        "        cache_dir=TEMP_DIR,\n",
        "        resume_download=True\n",
        "    )\n",
        "\n",
        "    shard_data = load_file(shard_path)\n",
        "    print(f\"   åŒ…å«å‚æ•°: {len(shard_data)} ä¸ª\")\n",
        "\n",
        "    for param_idx, (param_name, tensor) in enumerate(shard_data.items()):\n",
        "        tensor = tensor.cpu()\n",
        "\n",
        "        if should_quantize(param_name) and tensor.dim() >= 2:\n",
        "            # ğŸ”§ ä½¿ç”¨ Per-Channel é‡åŒ–\n",
        "            q_tensor, scale = quantize_tensor_int8_per_channel(tensor)\n",
        "\n",
        "            if scale is not None:\n",
        "                # ğŸ”§ å…³é”®ï¼šscale é”®åæ ¼å¼\n",
        "                scale_key = param_name.replace(\".weight\", \".scale\")\n",
        "                scales_dict[scale_key] = scale\n",
        "\n",
        "                quantized_count += 1\n",
        "\n",
        "                if param_idx < 3:\n",
        "                    print(f\"   âœ“ [{param_idx+1}] {param_name}\")\n",
        "                    print(f\"      é‡åŒ–: {list(tensor.shape)} â†’ int8\")\n",
        "                    print(f\"      scale shape: {list(scale.shape)} (per-channel)\")\n",
        "\n",
        "        else:\n",
        "            # ä¿æŒä¸º bfloat16\n",
        "            q_tensor = tensor.to(torch.bfloat16) if tensor.dtype != torch.bfloat16 else tensor\n",
        "            kept_count += 1\n",
        "\n",
        "            if param_idx < 3:\n",
        "                print(f\"   - [{param_idx+1}] {param_name}\")\n",
        "                print(f\"      ä¿æŒ: bfloat16\")\n",
        "\n",
        "        param_size = q_tensor.numel() * q_tensor.element_size()\n",
        "\n",
        "        # å†…å­˜æ§åˆ¶ï¼šè¾¾åˆ°ä¸´æ—¶åˆ†ç‰‡å¤§å°å°±ä¿å­˜\n",
        "        if current_temp_size + param_size > TEMP_SHARD_SIZE and current_temp_shard:\n",
        "            temp_file = os.path.join(TEMP_DIR, f\"temp_{temp_shard_idx:04d}.safetensors\")\n",
        "            print(f\"\\n   ğŸ’¾ æš‚å­˜ä¸´æ—¶åˆ†ç‰‡ #{temp_shard_idx} ({len(current_temp_shard)} å‚æ•°)\")\n",
        "            save_file(current_temp_shard, temp_file)\n",
        "\n",
        "            current_temp_shard.clear()\n",
        "            current_temp_size = 0\n",
        "            temp_shard_idx += 1\n",
        "            force_cleanup()\n",
        "\n",
        "        current_temp_shard[param_name] = q_tensor\n",
        "        current_temp_size += param_size\n",
        "\n",
        "        del tensor, q_tensor\n",
        "\n",
        "    del shard_data\n",
        "    force_cleanup()\n",
        "\n",
        "    try:\n",
        "        os.remove(shard_path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ä¿å­˜æœ€åä¸€ä¸ªä¸´æ—¶åˆ†ç‰‡\n",
        "if current_temp_shard:\n",
        "    temp_file = os.path.join(TEMP_DIR, f\"temp_{temp_shard_idx:04d}.safetensors\")\n",
        "    print(f\"\\nğŸ’¾ ä¿å­˜æœ€åä¸´æ—¶åˆ†ç‰‡ #{temp_shard_idx}\")\n",
        "    save_file(current_temp_shard, temp_file)\n",
        "    del current_temp_shard\n",
        "    force_cleanup()\n",
        "\n",
        "total_temp_shards = temp_shard_idx\n",
        "\n",
        "# ä¿å­˜ scales\n",
        "if scales_dict:\n",
        "    print(f\"\\nğŸ’¾ ä¿å­˜é‡åŒ– scales: {len(scales_dict)} ä¸ª\")\n",
        "\n",
        "    # ğŸ”§ éªŒè¯ï¼šæ£€æŸ¥ scale çš„ shape\n",
        "    sample_scale_keys = list(scales_dict.keys())[:3]\n",
        "    print(f\"   å‰ 3 ä¸ª scale çš„ shape:\")\n",
        "    for k in sample_scale_keys:\n",
        "        print(f\"      {k}: {list(scales_dict[k].shape)}\")\n",
        "\n",
        "    save_file(scales_dict, os.path.join(TEMP_DIR, \"temp_scales.safetensors\"))\n",
        "    del scales_dict\n",
        "    force_cleanup()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"âœ… é‡åŒ–ç»Ÿè®¡:\")\n",
        "print(f\"   é‡åŒ–å‚æ•°: {quantized_count}\")\n",
        "print(f\"   ä¿æŒå‚æ•°: {kept_count}\")\n",
        "print(f\"   ä¸´æ—¶åˆ†ç‰‡æ•°: {total_temp_shards}\")\n",
        "print(f\"{'='*60}\")\n",
        "print_mem()\n",
        "\n",
        "# ==========================================\n",
        "# 3. åˆå¹¶ä¸ºå•ä¸ªæ–‡ä»¶\n",
        "# ==========================================\n",
        "print(f\"\\nğŸ”— æ­¥éª¤ 3: åˆå¹¶ä¸ºå•ä¸ªæ–‡ä»¶...\")\n",
        "\n",
        "final_state_dict = {}\n",
        "\n",
        "for i in range(1, total_temp_shards + 1):\n",
        "    temp_file = os.path.join(TEMP_DIR, f\"temp_{i:04d}.safetensors\")\n",
        "\n",
        "    if not os.path.exists(temp_file):\n",
        "        continue\n",
        "\n",
        "    print(f\"   [{i}/{total_temp_shards}] åŠ è½½ä¸´æ—¶åˆ†ç‰‡...\")\n",
        "    temp_data = load_file(temp_file)\n",
        "    final_state_dict.update(temp_data)\n",
        "    del temp_data\n",
        "    force_cleanup()\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print_mem()\n",
        "\n",
        "# åŠ è½½ scales\n",
        "scales_file = os.path.join(TEMP_DIR, \"temp_scales.safetensors\")\n",
        "if os.path.exists(scales_file):\n",
        "    print(f\"   åŠ è½½ scales...\")\n",
        "    scales_data = load_file(scales_file)\n",
        "    final_state_dict.update(scales_data)\n",
        "    del scales_data\n",
        "    force_cleanup()\n",
        "\n",
        "print(f\"\\nâœ… åˆå¹¶ç»Ÿè®¡:\")\n",
        "print(f\"   æ€»å‚æ•°æ•°: {len(final_state_dict)}\")\n",
        "\n",
        "# éªŒè¯\n",
        "scale_keys = [k for k in final_state_dict.keys() if k.endswith('.scale')]\n",
        "weight_keys = [k for k in final_state_dict.keys() if k.endswith('.weight')]\n",
        "\n",
        "print(f\"   æƒé‡å‚æ•°: {len(weight_keys)}\")\n",
        "print(f\"   Scale å‚æ•°: {len(scale_keys)}\")\n",
        "\n",
        "# ğŸ”§ éªŒè¯ scale çš„ shape\n",
        "print(f\"\\n   éªŒè¯ scale shapeï¼ˆå‰ 3 ä¸ªï¼‰:\")\n",
        "for k in sorted(scale_keys)[:3]:\n",
        "    scale = final_state_dict[k]\n",
        "    print(f\"      {k}: {list(scale.shape)}\")\n",
        "\n",
        "# ä¿å­˜å•æ–‡ä»¶\n",
        "print(f\"\\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ–‡ä»¶...\")\n",
        "\n",
        "save_file(\n",
        "    final_state_dict,\n",
        "    os.path.join(LOCAL_DIR, \"diffusion_pytorch_model.safetensors\"),\n",
        "    metadata={\"format\": \"pt\"}\n",
        ")\n",
        "\n",
        "file_size = os.path.getsize(os.path.join(LOCAL_DIR, \"diffusion_pytorch_model.safetensors\"))\n",
        "print(f\"   âœ“ æ–‡ä»¶å¤§å°: {file_size/1e9:.2f} GB\")\n",
        "\n",
        "del final_state_dict\n",
        "force_cleanup()\n",
        "print_mem()\n",
        "\n",
        "# ==========================================\n",
        "# 4. ç”Ÿæˆé…ç½®æ–‡ä»¶\n",
        "# ==========================================\n",
        "print(f\"\\nğŸ“ æ­¥éª¤ 4: ç”Ÿæˆé…ç½®æ–‡ä»¶...\")\n",
        "\n",
        "quantization_config_standalone = {\n",
        "    \"add_skip_keys\": False,\n",
        "    \"dequantize_fp32\": False,\n",
        "    \"group_size\": -1,\n",
        "    \"is_integer\": True,\n",
        "    \"is_training\": False,\n",
        "    \"modules_dtype_dict\": {},\n",
        "    \"modules_to_not_convert\": target_modules_to_not_convert,\n",
        "    \"non_blocking\": False,\n",
        "    \"quant_conv\": False,\n",
        "    \"quant_method\": \"sdnq\",\n",
        "    \"quantization_device\": None,\n",
        "    \"quantized_matmul_dtype\": None,\n",
        "    \"return_device\": None,\n",
        "    \"sdnq_version\": \"0.1.2\",\n",
        "    \"svd_rank\": 32,\n",
        "    \"svd_steps\": 8,\n",
        "    \"use_grad_ckpt\": True,\n",
        "    \"use_quantized_matmul\": False,\n",
        "    \"use_quantized_matmul_conv\": False,\n",
        "    \"use_static_quantization\": True,\n",
        "    \"use_stochastic_rounding\": False,\n",
        "    \"use_svd\": False,\n",
        "    \"weights_dtype\": \"int8\"\n",
        "}\n",
        "\n",
        "with open(os.path.join(LOCAL_DIR, \"quantization_config.json\"), \"w\") as f:\n",
        "    json.dump(quantization_config_standalone, f, indent=2)\n",
        "\n",
        "quantization_config_embedded = {\n",
        "    **quantization_config_standalone,\n",
        "    \"quantization_device\": \"xpu\",\n",
        "    \"return_device\": \"cpu\",\n",
        "    \"add_skip_keys\": True,\n",
        "}\n",
        "\n",
        "base_config[\"quantization_config\"] = quantization_config_embedded\n",
        "base_config[\"_diffusers_version\"] = \"0.36.0.dev0\"\n",
        "base_config.pop(\"siglip_feat_dim\", None)\n",
        "\n",
        "with open(os.path.join(LOCAL_DIR, \"config.json\"), \"w\") as f:\n",
        "    json.dump(base_config, f, indent=2)\n",
        "\n",
        "print(\"   âœ“ é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. æ‹·è´åˆ° Drive\n",
        "# ==========================================\n",
        "print(f\"\\nğŸ“¤ æ­¥éª¤ 5: æ‹·è´åˆ° Drive...\")\n",
        "print(f\"   ç›®æ ‡: {FINAL_DIR}\")\n",
        "\n",
        "if os.path.exists(FINAL_DIR):\n",
        "    shutil.rmtree(FINAL_DIR)\n",
        "\n",
        "shutil.copytree(LOCAL_DIR, FINAL_DIR)\n",
        "print(\"   âœ“ æ‹·è´å®Œæˆ\")\n",
        "\n",
        "print(f\"\\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...\")\n",
        "shutil.rmtree(LOCAL_DIR, ignore_errors=True)\n",
        "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
        "force_cleanup()\n",
        "\n",
        "# ==========================================\n",
        "# 6. æœ€ç»ˆéªŒè¯\n",
        "# ==========================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ğŸ‰ Per-Channel é‡åŒ–å®Œæˆï¼\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(f\"\\nğŸ“ æœ€ç»ˆæ–‡ä»¶:\")\n",
        "for f in sorted(os.listdir(FINAL_DIR)):\n",
        "    fpath = os.path.join(FINAL_DIR, f)\n",
        "    if os.path.isfile(fpath):\n",
        "        fsize = os.path.getsize(fpath)\n",
        "        if fsize > 1e6:\n",
        "            print(f\"   âœ“ {f:<45} {fsize/1e9:>8.2f} GB\")\n",
        "        else:\n",
        "            print(f\"   âœ“ {f:<45} {fsize/1e3:>8.1f} KB\")\n",
        "\n",
        "print(f\"\\nâœ… ä¸ Turbo å¯¹é½æ£€æŸ¥:\")\n",
        "print(f\"   âœ“ é‡åŒ–æ–¹å¼: Per-Channel\")\n",
        "print(f\"   âœ“ Scale shape: [out_features, 1]\")\n",
        "print(f\"   âœ“ Scale dtype: bfloat16\")\n",
        "print(f\"   âœ“ æœªé‡åŒ–å±‚: bfloat16\")\n",
        "print(f\"   âœ“ é‡åŒ–å‚æ•°æ•°: {quantized_count}\")\n",
        "\n",
        "print(f\"\\nğŸš€ ä½¿ç”¨æ–¹æ³•:\")\n",
        "print(f\"   !cp -rf {FINAL_DIR}/* \\\\\")\n",
        "print(f\"           /content/Z-Image-Turbo-SDNQ-int8/transformer/\")\n",
        "\n",
        "print_mem()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7CIj6FBMFiDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}